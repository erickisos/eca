{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#eca-editor-code-assistant","title":"ECA (Editor Code Assistant)","text":"eca-emacs eca-vscode eca-intellij <p> installation \u2022   features \u2022   configuration \u2022   models \u2022   protocol troubleshooting </p> <ul> <li> Editor-agnostic: protocol for any editor to integrate.</li> <li> Single configuration: Configure eca making it work the same in any editor via global or local configs.</li> <li> Chat interface: ask questions, review code, work together to code.</li> <li> Agentic: let LLM work as an agent with its native tools and MCPs you can configure.</li> <li> Context: support: giving more details about your code to the LLM, including MCP resources and prompts.</li> <li> Multi models: Login to OpenAI, Anthropic, Copilot, Ollama local models and many more.</li> <li> OpenTelemetry: Export metrics of tools, prompts, server usage.</li> </ul>"},{"location":"#rationale","title":"Rationale","text":"<p>A Free and OpenSource editor-agnostic tool that aims to easily link LLMs &lt;-&gt; Editors, giving the best UX possible for AI pair programming using a well-defined protocol. The server is written in Clojure and heavily inspired by the LSP protocol which is a success case for this kind of integration.</p> <p>The protocol makes it easier for other editors to integrate, and having a server in the middle helps add more features quickly, some examples: - Tool call management - Multiple LLM interaction  - Telemetry of feature usage - Single way to configure for any editor - Same UX, easy to onboard people and teams. </p> <p>With the LLMs models race, the differences between them tend to be irrelevant in the future, but UX on how to edit code or plan changes is something that will exist; ECA helps editors focus on that.</p> <p>How it works: Editors spawn the server via <code>eca server</code> and communicate via stdin/stdout, similar to LSPs. Supported editors already download the latest server on start and require no extra configuration.</p>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#1-install-the-editor-plugin","title":"1. Install the editor plugin","text":"<p>Install the plugin for your editor and ECA server will be downloaded and started automatically:</p> <ul> <li>Emacs</li> <li>VsCode</li> <li>Vim</li> <li>Intellij</li> </ul>"},{"location":"#2-set-up-your-first-model","title":"2. Set up your first model","text":"<p>To use ECA, you need to configure at least one model / provider (tip: Github Copilot offer free models!).</p> <p>See the Models documentation for detailed instructions:</p> <ol> <li>Type in the chat <code>/login</code>.</li> <li>Choose your provider</li> <li>Follow the steps to configure the key or auth for your provider.</li> <li>This will add to the global config.json the config for that provider.</li> </ol> <p>or configure manually.</p> <p>Note: For other providers or custom models, see the custom providers documentation.</p>"},{"location":"#3-start-chatting-completing-rewriting","title":"3. Start chatting, completing, rewriting","text":"<p>Once your model is configured, you can start using ECA's features interface in your editor to ask questions, review code, and work together on your project.</p> <p>Type <code>/init</code> to ask ECA to create/update a AGENTS.md file, which will help ECA in the next iterations have good context about your project standards.</p>"},{"location":"#roadmap","title":"Roadmap","text":"<p>Check the planned work here.</p>"},{"location":"#be-the-first-to-sponsor-the-project","title":"Be the first to sponsor the project \ud83d\udc96","text":"<p>Consider sponsoring the project to help grow faster, the support helps to keep the project going, being updated and maintained!</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are very welcome, please open an issue for discussion or a pull request. For developer details, check development docs.</p> <p>These are all the incredible people who helped make ECA better!</p> <p></p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG/#0921","title":"0.92.1","text":"<ul> <li>Add <code>x-llm-application-name: eca</code> to prompt requests, useful to track and get metrics when using LLM gateways.</li> </ul>"},{"location":"CHANGELOG/#0920","title":"0.92.0","text":"<ul> <li>Fix Gemini (OpenAI compatible). #247</li> <li>Improve login wording + add cancel option. #205</li> </ul>"},{"location":"CHANGELOG/#0912","title":"0.91.2","text":"<ul> <li>Fix <code>eca_shell_command</code> to include stderr output even when exit 0.</li> </ul>"},{"location":"CHANGELOG/#0911","title":"0.91.1","text":"<ul> <li>Fix openai pro login exceptions related to graalvm.</li> </ul>"},{"location":"CHANGELOG/#0910","title":"0.91.0","text":"<ul> <li>Codex login support via <code>/login openai</code> and selecting <code>pro</code> option. #261</li> <li>Add <code>gpt-5.2-codex</code> model.</li> </ul>"},{"location":"CHANGELOG/#0901","title":"0.90.1","text":"<ul> <li>Fix tokens not renewing between tool calls. #258</li> <li>Fix diff correct numbers in added/removed. #259</li> <li>Fix post tool call hook trigger</li> <li>Fix Gemini tool calling. #247</li> </ul>"},{"location":"CHANGELOG/#0900","title":"0.90.0","text":"<ul> <li>Skills support following https://agentskills.io . #241</li> <li>Make skills available via commands as well e.g. <code>/my-skill</code>.</li> <li>new command <code>/skills</code> to list available skills.</li> </ul>"},{"location":"CHANGELOG/#0890","title":"0.89.0","text":"<ul> <li>Support http MCP servers that require oauth. #51</li> <li>Add basic username/password proxy authentication support and recognize lowercase http[s]_proxy env var alongside HTTP[S]_PROXY. #248</li> <li>Avoid tool call of invalid names</li> </ul>"},{"location":"CHANGELOG/#0880","title":"0.88.0","text":"<ul> <li>Add dynamic model discovery via <code>fetchModels</code> provider config for OpenAI-compatible <code>/models</code> endpoints</li> <li>Improve error handling for incompatible models messages in chat. #209</li> <li>Support <code>server__tool_name</code> in <code>disabledTools</code> config as well.</li> <li>Fix clojure-mcp regression where ECA could not edit files via clojure-mcp even reading before using its tools.</li> </ul>"},{"location":"CHANGELOG/#0872","title":"0.87.2","text":"<ul> <li>Fix openai-chat tool call + support for Mistral API #233</li> <li>Skip missing/unreadable @file references when building context</li> <li>Fix regression: /compact not working for some models. Related to #240</li> </ul>"},{"location":"CHANGELOG/#0871","title":"0.87.1","text":"<ul> <li>Improve read-file summary to show final range properly.</li> <li>Improve model capabilities for providers which model name has slash: <code>my-provider/anthropic/my-model</code></li> </ul>"},{"location":"CHANGELOG/#0870","title":"0.87.0","text":"<ul> <li>Support Google Gemini thought signatures.</li> <li>Support <code>gemini-3-pro-preview</code> and <code>gemini-3-flash-preview</code> models in Google and Copilot providers.</li> <li>Fix deepseek reasoning with openai-chat API #228</li> <li>Support <code>~</code> in dynamic string parser.</li> <li>Support removing nullable values from LLM request body if the value in extraPayload is null. #232</li> </ul>"},{"location":"CHANGELOG/#0860","title":"0.86.0","text":"<ul> <li>Improve agent behavior prompt to mention usage of editor_diagnostics tool. #230</li> <li>Use selmer syntax for prompt templates.</li> </ul>"},{"location":"CHANGELOG/#0853","title":"0.85.3","text":"<ul> <li>Support <code>openai/gpt-5.2</code> and <code>github-copilot/gpt-5.2</code> by default.</li> </ul>"},{"location":"CHANGELOG/#0852","title":"0.85.2","text":"<ul> <li>Support <code>providers &lt;provider&gt; httpClient version</code> config, allowing to use http-1.1 for some providers like lmstudio. #229</li> </ul>"},{"location":"CHANGELOG/#0851","title":"0.85.1","text":"<ul> <li>Fix backwards compatibility for chat rollback.</li> </ul>"},{"location":"CHANGELOG/#0850","title":"0.85.0","text":"<ul> <li> <p>Enhanced hooks documentation with new types (sessionStart, sessionEnd, chatStart, chatEnd), JSON input/output schemas, execution options (timeout)</p> </li> <li> <p>Fix custom tools to support argument numbers.</p> </li> <li>Improve read_file summary to mention offset being read.</li> <li>Enhanced hooks documentation with new types (sessionStart, sessionEnd, chatStart, chatEnd), JSON input/output schemas, execution options (timeout)</li> <li>Support rollback only messages, tool call changes or both in <code>chat/rollback</code>.</li> </ul>"},{"location":"CHANGELOG/#0842","title":"0.84.2","text":"<ul> <li>Fix <code>${netrc:...}</code> to consider <code>:netrcFile</code> config properly.</li> </ul>"},{"location":"CHANGELOG/#0841","title":"0.84.1","text":"<ul> <li>Fix <code>${netrc:...}</code> to consider <code>:netrcFile</code> config.</li> </ul>"},{"location":"CHANGELOG/#0840","title":"0.84.0","text":"<ul> <li>Improve <code>/compact</code> UI in chat after running, cleaning chat and showing the new summary.</li> <li>Better config values dynamic string parse:<ul> <li>Support <code>${classapath:path/to/eca/classpath/file}</code> in dynamic string parse.</li> <li>Support <code>${netrc:api.foo.com}</code> in dynamic string parse to parse keys. #200</li> <li>Support default env values in <code>${env:MY_ENV:default-value}</code>.</li> <li>Support for ECA_CONFIG and custom config file.</li> </ul> </li> <li>Deprecate configs:</li> <li><code>systemPromptFile</code> in favor of <code>systemPrompt</code> using <code>${file:...}</code> or <code>${classpath:...}</code></li> <li><code>urlEnv</code> in favor of <code>url</code> using <code>${env:...}</code></li> <li><code>keyEnv</code> in favor of <code>key</code> using <code>${env:...}</code></li> <li><code>keyRc</code> in favor of <code>key</code> using <code>${netrc:...}</code></li> <li><code>compactPromptFile</code> in favor of <code>compactPrompt</code> using <code>${classpath:...}</code></li> </ul>"},{"location":"CHANGELOG/#0830","title":"0.83.0","text":"<ul> <li>Support dynamic string parse (<code>${file:/path/to/something}</code> and <code>${env:MY_ENV}</code>) in all configs with string values. #200</li> </ul>"},{"location":"CHANGELOG/#0821","title":"0.82.1","text":"<ul> <li>Fix custom tools output to return stderr when tool error. #219</li> </ul>"},{"location":"CHANGELOG/#0820","title":"0.82.0","text":"<ul> <li>Support nested folder for rules and commands. #220</li> </ul>"},{"location":"CHANGELOG/#0810","title":"0.81.0","text":"<ul> <li>Support rollback file changes done by <code>write_file</code>, <code>edit_file</code> and <code>move_file</code>. #218</li> <li>Improve rollback to keep consistent UI before the rollback, fixing tool names and user messages.</li> </ul>"},{"location":"CHANGELOG/#0804","title":"0.80.4","text":"<ul> <li>Fix binary for macos amd64. #217</li> </ul>"},{"location":"CHANGELOG/#0803","title":"0.80.3","text":"<ul> <li>Fix release</li> </ul>"},{"location":"CHANGELOG/#0802","title":"0.80.2","text":"<ul> <li>Update anthropic default models to include opus-4.5</li> <li>Update anthropic default models to use alias names.</li> </ul>"},{"location":"CHANGELOG/#0801","title":"0.80.1","text":"<ul> <li>Add new models to GitHub config (Gpt 5.1 and Opus 4.5).</li> </ul>"},{"location":"CHANGELOG/#0800","title":"0.80.0","text":"<ul> <li>Add support to rollback messages via <code>chat/rollback</code> and <code>chat/clear</code> messages. #42</li> </ul>"},{"location":"CHANGELOG/#0791","title":"0.79.1","text":"<ul> <li>Improve system prompt to add project env context.</li> </ul>"},{"location":"CHANGELOG/#0790","title":"0.79.0","text":"<ul> <li>Fix absolute paths being interpreted as commands. #199</li> <li>Remove non used sync models code during initialize. #100</li> <li>Fix system prompt to mention the user workspace roots.</li> </ul>"},{"location":"CHANGELOG/#0784","title":"0.78.4","text":"<ul> <li>Fix regression exceptions on specific corner cases with log obfuscation.</li> </ul>"},{"location":"CHANGELOG/#0783","title":"0.78.3","text":"<ul> <li>Add <code>openai/gpt-5.1</code> to default models.</li> </ul>"},{"location":"CHANGELOG/#0782","title":"0.78.2","text":"<ul> <li>Add workspaces to <code>/doctor</code></li> <li>Improve LLM request logs to include headers.</li> </ul>"},{"location":"CHANGELOG/#0781","title":"0.78.1","text":"<ul> <li>Fix regression: completion broken after rewrite feature API changes.</li> </ul>"},{"location":"CHANGELOG/#0780","title":"0.78.0","text":"<ul> <li>Prefix tool name with server to LLM: __. #196 <li>Remove <code>eca_</code> prefix from eca tools, we already pass server prefix (eca) after #196.</li> <li>Add <code>approval</code> arg to preToolCall hook input.</li> <li>Add error rewrite message content message.</li>"},{"location":"CHANGELOG/#0771","title":"0.77.1","text":"<ul> <li>Fix token renew when using rewrite feature.</li> <li>Improve rewrite error handling.</li> </ul>"},{"location":"CHANGELOG/#0770","title":"0.77.0","text":"<ul> <li>Custom providers do not require the existense of <code>key</code> or <code>keyEnv</code>. #194</li> <li>New feature: rewrite. #13</li> </ul>"},{"location":"CHANGELOG/#0760","title":"0.76.0","text":"<ul> <li>Updated instructions for <code>/login</code> command and invalid input handling.</li> <li>Fix server name on <code>chat/contentReceived</code> when preparing tool call.</li> <li>Fix variable replacing in some tool prompts.</li> <li>Improve planning mode prompt and tool docs; clarify absolute-path usage and preview rules.</li> <li>Centralize path approval for tools and always list all missing required params in INVALID_ARGS.</li> </ul>"},{"location":"CHANGELOG/#0754","title":"0.75.4","text":"<ul> <li>Fix 0.75.3 regression on custom openai-chat providers.</li> </ul>"},{"location":"CHANGELOG/#0753","title":"0.75.3","text":"<ul> <li>Support custom think tag start and end for openai-chat models via <code>think-tag-start</code> and <code>think-tag-end</code> provider configs. #188</li> <li>Bump MCP java sdk to 0.15.0.</li> </ul>"},{"location":"CHANGELOG/#0752","title":"0.75.2","text":"<ul> <li>Add missing models supported by Github Copilot</li> <li>Fix regression: openai-chat tool call arguments error on some models.</li> </ul>"},{"location":"CHANGELOG/#0751","title":"0.75.1","text":"<ul> <li>Improve protocol for tool call output formatting for tools that output json.</li> <li>Fix inconsistencies in <code>eca_read_file</code> not passing correct content to LLM when json.</li> </ul>"},{"location":"CHANGELOG/#0750","title":"0.75.0","text":"<ul> <li>Improved file contexts: now use :lines-range</li> <li>BREAKING ECA now only supports standard plain-text netrc as credential file reading. Drop authinfo and gpg decryption support. Users can choose to pass in their own provisioned netrc file from various secure source with <code>:netrcFile</code> in ECA config.</li> </ul>"},{"location":"CHANGELOG/#0740","title":"0.74.0","text":"<ul> <li>Improved <code>eca_edit_file</code> to automatically handle whitespace and indentation differences in single-occurrence edits.</li> <li>Fix contexts in user prompts (not system contexts) not parsing lines ranges properly.</li> <li>Support non-stream providers on openai-chat API. #174</li> </ul>"},{"location":"CHANGELOG/#0735","title":"0.73.5","text":"<ul> <li>Support use API keys even if subscription is logged. #175</li> </ul>"},{"location":"CHANGELOG/#0734","title":"0.73.4","text":"<ul> <li>Fix tool call approval ignoring eca tools.</li> </ul>"},{"location":"CHANGELOG/#0733","title":"0.73.3","text":"<ul> <li>Fix tool call approval ignoring configs for mcp servers.</li> </ul>"},{"location":"CHANGELOG/#0732","title":"0.73.2","text":"<ul> <li>Fix tool call approval thread lock.</li> </ul>"},{"location":"CHANGELOG/#0731","title":"0.73.1","text":"<ul> <li>Improve chat title generation.</li> <li>Fix completion error handling.</li> <li>Default to <code>openai/gpt-4.1</code> on completion.</li> </ul>"},{"location":"CHANGELOG/#0730","title":"0.73.0","text":"<ul> <li>Add <code>:config-file</code> cli option to pass in config.</li> <li>Add support for completion. #12</li> </ul>"},{"location":"CHANGELOG/#0722","title":"0.72.2","text":"<ul> <li>Run <code>preToolCall</code> hook before user approval if any. #170</li> </ul>"},{"location":"CHANGELOG/#0721","title":"0.72.1","text":"<ul> <li>Only include <code>parallel_tool_calls</code> to openai-responses and openai-chat if true. #169</li> </ul>"},{"location":"CHANGELOG/#0720","title":"0.72.0","text":"<ul> <li>Support clojureMCP dry-run flags for edit/write tools, being able to show preview of diffs before running tool.</li> </ul>"},{"location":"CHANGELOG/#0713","title":"0.71.3","text":"<ul> <li>Assoc <code>parallel_tool_calls</code> to openai-chat only if truth.</li> </ul>"},{"location":"CHANGELOG/#0712","title":"0.71.2","text":"<ul> <li>Fix regression in <code>/compact</code> command. #162</li> <li>Fix to use local zone for time presentation in <code>/resume</code>.</li> </ul>"},{"location":"CHANGELOG/#0711","title":"0.71.1","text":"<ul> <li>Use web-search false if model capabiltiies are not found.</li> </ul>"},{"location":"CHANGELOG/#0710","title":"0.71.0","text":"<ul> <li>Support <code>/resume</code> a specific chat.</li> </ul>"},{"location":"CHANGELOG/#0706","title":"0.70.6","text":"<ul> <li>Fix <code>openai-chat</code> api not following <code>completionUrlRelativePath</code>.</li> </ul>"},{"location":"CHANGELOG/#0705","title":"0.70.5","text":"<ul> <li>Fix web-search not working for custom models using openai/anthropic apis.</li> </ul>"},{"location":"CHANGELOG/#0704","title":"0.70.4","text":"<ul> <li>Support <code>visible</code> field in hooks configuration to show or not in client.</li> </ul>"},{"location":"CHANGELOG/#0703","title":"0.70.3","text":"<ul> <li>Deprecate prePrompt and postPrompt in favor of preRequest and prePrompt.</li> </ul>"},{"location":"CHANGELOG/#0702","title":"0.70.2","text":"<ul> <li>Fix model capabilities for models with custom names.</li> </ul>"},{"location":"CHANGELOG/#0701","title":"0.70.1","text":"<ul> <li>Fix prePrompt hook.</li> </ul>"},{"location":"CHANGELOG/#0700","title":"0.70.0","text":"<ul> <li>Add hooks support. #43</li> </ul>"},{"location":"CHANGELOG/#0691","title":"0.69.1","text":"<ul> <li>Fix regression on models with no extraPayload.</li> </ul>"},{"location":"CHANGELOG/#0690","title":"0.69.0","text":"<ul> <li>Support multiple model configs with different payloads using same model name via <code>modelName</code> config. (Ex: gpt-5 and gpt-5-high but both use gpt-5)</li> </ul>"},{"location":"CHANGELOG/#0681","title":"0.68.1","text":"<ul> <li>Add <code>anthropic/haiku-4.5</code> model by default.</li> </ul>"},{"location":"CHANGELOG/#0680","title":"0.68.0","text":"<ul> <li>Unwrap mentioned @contexts in prompt appending as user message its content. #154</li> </ul>"},{"location":"CHANGELOG/#0670","title":"0.67.0","text":"<ul> <li>Improved flaky test #150</li> <li>Obfuscate env vars in /doctor.</li> <li>Bump clj-otel to 0.2.10</li> <li>Rename $ARGS to $ARGUMENTS placeholder alias for custom commands.</li> <li>Support recursive AGENTS.md file inclusions with @file mention. #140</li> </ul>"},{"location":"CHANGELOG/#0661","title":"0.66.1","text":"<ul> <li>Improve plan behavior prompt. #139</li> </ul>"},{"location":"CHANGELOG/#0660","title":"0.66.0","text":"<ul> <li>Add support for secrets stored in authinfo and netrc files</li> <li>Added tests for stopping concurrent tool calls. #147</li> <li>Improve logging.</li> <li>Improve performance of <code>chat/queryContext</code>.</li> </ul>"},{"location":"CHANGELOG/#0650","title":"0.65.0","text":"<ul> <li>Added ability to cancel tool calls. Only the shell tool currently. #145</li> <li>Bump mcp java sdk to 0.14.1.</li> <li>Improve json output for tools that output json.</li> </ul>"},{"location":"CHANGELOG/#0641","title":"0.64.1","text":"<ul> <li>Fix duplicated arguments on <code>toolCallPrepare</code> for openai-chat API models. https://github.com/editor-code-assistant/eca-emacs/issues/56</li> </ul>"},{"location":"CHANGELOG/#0640","title":"0.64.0","text":"<ul> <li>Add <code>server</code> to tool call messages.</li> </ul>"},{"location":"CHANGELOG/#0633","title":"0.63.3","text":"<ul> <li>Fix last word going after tool call for openai-chat API.</li> </ul>"},{"location":"CHANGELOG/#0632","title":"0.63.2","text":"<ul> <li>Fix retrocompatibility with some models not working with openai-chat like deepseek.</li> </ul>"},{"location":"CHANGELOG/#0631","title":"0.63.1","text":"<ul> <li>Add <code>gpt-5-codex</code> model as default for <code>openai</code> provider.</li> </ul>"},{"location":"CHANGELOG/#0630","title":"0.63.0","text":"<ul> <li>Support \"accept and remember\" tool call per session and name.</li> <li>Avoid generating huge chat titles.</li> </ul>"},{"location":"CHANGELOG/#0621","title":"0.62.1","text":"<ul> <li>Add <code>claude-sonnet-4.5</code> for github-copilot provider.</li> <li>Add <code>prompt-received</code> metric.</li> </ul>"},{"location":"CHANGELOG/#0620","title":"0.62.0","text":"<ul> <li>Use a default of 32k tokens for max_tokens in openai-chat API.</li> <li>Improve rejection prompt for tool calls.</li> <li>Use <code>max_completion_tokens</code> instead of <code>max_tokens</code> in openai-chat API.</li> <li>Support context/tokens usage/cost for openai-chat API.</li> <li>Support <code>anthropic/claude-sonnet-4.5</code> by default.</li> </ul>"},{"location":"CHANGELOG/#0611","title":"0.61.1","text":"<ul> <li>More tolerant whitespace handling after <code>data:</code>.</li> <li>Fix login for google provider. #134</li> </ul>"},{"location":"CHANGELOG/#0610","title":"0.61.0","text":"<ul> <li>Fix chat titles not working for some providers.</li> <li>Enable reasoning for google models.</li> <li>Support reasoning blocks in models who use openai-chat api.</li> </ul>"},{"location":"CHANGELOG/#0600","title":"0.60.0","text":"<ul> <li>Support google gemini as built-in models. #50</li> </ul>"},{"location":"CHANGELOG/#0590","title":"0.59.0","text":"<ul> <li>Deprecate repoMap context, will be removed in the future.</li> <li>After lots of tunnings and improvements, the repoMap is no longer relevant as <code>eca_directory_tree</code> provides similar and more specific view for LLM to use.</li> <li>Support <code>toolCall shellCommand summaryMaxLength</code> to configure UX of command length. #130</li> </ul>"},{"location":"CHANGELOG/#0582","title":"0.58.2","text":"<ul> <li>Fix MCP prompt for native image.</li> </ul>"},{"location":"CHANGELOG/#0581","title":"0.58.1","text":"<ul> <li>Improve progress notification when tool is running.</li> </ul>"},{"location":"CHANGELOG/#0580","title":"0.58.0","text":"<ul> <li>Bump MCP java sdk to 0.13.1</li> <li>Improve MCP logs on stderr.</li> <li>Support tool call rejection with reasons inputed by user. #127</li> </ul>"},{"location":"CHANGELOG/#0570","title":"0.57.0","text":"<ul> <li>Greatly reduce token consuming of <code>eca_directory_tree</code></li> <li>Ignoring files in gitignore</li> <li>Improving tool output for LLM removing token consuming chars.</li> </ul>"},{"location":"CHANGELOG/#0564","title":"0.56.4","text":"<ul> <li>Fix renew oauth tokens when it expires in the same session.</li> </ul>"},{"location":"CHANGELOG/#0563","title":"0.56.3","text":"<ul> <li>Fix metrics exception when saving to db.</li> </ul>"},{"location":"CHANGELOG/#0562","title":"0.56.2","text":"<ul> <li>Fix db exception.</li> </ul>"},{"location":"CHANGELOG/#0561","title":"0.56.1","text":"<ul> <li>Fix usage reporting.</li> </ul>"},{"location":"CHANGELOG/#0560","title":"0.56.0","text":"<ul> <li>Return new chat metadata content.</li> <li>Add chat title via prompt to LLM.</li> </ul>"},{"location":"CHANGELOG/#0550","title":"0.55.0","text":"<ul> <li>Add support for Opentelemetry via <code>otlp</code> config.</li> <li>Export metrics of server tasks, tool calls, prompts, resources.</li> </ul>"},{"location":"CHANGELOG/#0544","title":"0.54.4","text":"<ul> <li>Use jsonrpc4clj instead of lsp4clj.</li> <li>Bump graalvm to 24 and java to 24 improving native binary perf.</li> </ul>"},{"location":"CHANGELOG/#0543","title":"0.54.3","text":"<ul> <li>Avoid errors on multiple same MCP server calls in parallel.</li> </ul>"},{"location":"CHANGELOG/#0542","title":"0.54.2","text":"<ul> <li>Fix openai cache tokens cost calculation.</li> </ul>"},{"location":"CHANGELOG/#0541","title":"0.54.1","text":"<ul> <li>Improve welcome message.</li> </ul>"},{"location":"CHANGELOG/#0540","title":"0.54.0","text":"<ul> <li>Improve large file handling in <code>read-file</code> tool:</li> <li>Replace basic truncation notice with detailed line range information and next-step instructions.</li> <li>Allow users to customize default line limit through <code>tools.readFile.maxLines</code> configuration (keep the current 2000 as default).</li> <li>Moved the future in :on-tools-called and stored it in the db. #119</li> <li>Support <code>compactPromptFile</code> config.</li> <li>Fix tools not being listed for servers using mcp-remote.</li> </ul>"},{"location":"CHANGELOG/#0530","title":"0.53.0","text":"<ul> <li>Add <code>/compact</code> command to summarize the current conversation helping reduce context size.</li> <li>Add support for images as contexts.</li> </ul>"},{"location":"CHANGELOG/#0520","title":"0.52.0","text":"<ul> <li>Support http-streamable http servers (non auth support for now)</li> <li>Fix promtps that send assistant messages not working for anthropic.</li> </ul>"},{"location":"CHANGELOG/#0513","title":"0.51.3","text":"<ul> <li>Fix manual anthropic login to save credentials in global config instead of cache.</li> </ul>"},{"location":"CHANGELOG/#0512","title":"0.51.2","text":"<ul> <li>Minor log improvement of failed to start MCPs.</li> </ul>"},{"location":"CHANGELOG/#0511","title":"0.51.1","text":"<ul> <li>Bump mcp java sdk to 1.12.1.</li> <li>Fix mcp servers default timeout from 20s -&gt; 60s.</li> </ul>"},{"location":"CHANGELOG/#0510","title":"0.51.0","text":"<ul> <li>Support timeout on <code>eca_shell_command</code> with default to 1min.</li> <li>Support <code>@cursor</code> context representing the current editor cursor position. #103</li> </ul>"},{"location":"CHANGELOG/#0502","title":"0.50.2","text":"<ul> <li>Fix setting the <code>web-search</code> capability in the relevant models</li> <li>Fix summary text for tool calls using <code>openai-chat</code> api.</li> </ul>"},{"location":"CHANGELOG/#0501","title":"0.50.1","text":"<ul> <li>Bump mcp-java-sdk to 0.12.0.</li> </ul>"},{"location":"CHANGELOG/#0500","title":"0.50.0","text":"<ul> <li>Added missing parameters to <code>toolCallRejected</code> where possible.  PR #109</li> <li>Improve plan prompt present plan step.</li> <li>Add custom behavior configuration support. #79</li> <li>Behaviors can now define <code>defaultModel</code>, <code>disabledTools</code>, <code>systemPromptFile</code>, and <code>toolCall</code> approval rules.</li> <li>Built-in <code>agent</code> and <code>plan</code> behaviors are pre-configured.</li> <li>Replace <code>systemPromptTemplateFile</code> with <code>systemPromptFile</code> for complete prompt files instead of templates.</li> <li>Remove <code>nativeTools</code> configuration in favor of <code>toolCall</code> approval and <code>disabledTools</code>.</li> <li>Native tools are now always enabled by default, controlled via <code>disabledTools</code> and <code>toolCall</code> approval.</li> </ul>"},{"location":"CHANGELOG/#0490","title":"0.49.0","text":"<ul> <li>Add <code>totalTimeMs</code> to reason and toolCall content blocks.</li> </ul>"},{"location":"CHANGELOG/#0480","title":"0.48.0","text":"<ul> <li>Add nix flake build.</li> <li>Stop prompt does not change the status of the last running toolCall. #65</li> <li>Add <code>toolCallRunning</code> content to <code>chat/contentReceived</code>.</li> </ul>"},{"location":"CHANGELOG/#0470","title":"0.47.0","text":"<ul> <li>Support more providers login via <code>/login</code>.</li> <li>openai</li> <li>openrouter</li> <li>deepseek</li> <li>azure</li> <li>z-ai</li> </ul>"},{"location":"CHANGELOG/#0460","title":"0.46.0","text":"<ul> <li>Remove the need to pass <code>requestId</code> on prompt messages.</li> <li>Support empty <code>/login</code> command to ask what provider to login.</li> </ul>"},{"location":"CHANGELOG/#0450","title":"0.45.0","text":"<ul> <li>Support user configured custom tools via <code>customTools</code> config. #92</li> <li>Fix default approval for read only tools to be <code>allow</code> instead of <code>ask</code>.</li> </ul>"},{"location":"CHANGELOG/#0441","title":"0.44.1","text":"<ul> <li>Fix renew token regression.</li> <li>Improve error feedback when failed to renew token.</li> </ul>"},{"location":"CHANGELOG/#0440","title":"0.44.0","text":"<ul> <li>Support <code>deny</code> tool calls via <code>toolCall approval deny</code> setting.</li> </ul>"},{"location":"CHANGELOG/#0431","title":"0.43.1","text":"<ul> <li>Safely rename <code>default*</code> -&gt; <code>select*</code> in <code>config/updated</code>.</li> </ul>"},{"location":"CHANGELOG/#0430","title":"0.43.0","text":"<ul> <li>Support <code>chat/selectedBehaviorChanged</code> client notification.</li> <li>Update models according with supported models given its auth or key/url configuration.</li> <li>Return models only authenticated or logged in avoid too much models on UI that won't work.</li> </ul>"},{"location":"CHANGELOG/#0420","title":"0.42.0","text":"<ul> <li>New server notification <code>config/updated</code> used to notify clients when a relevant config changed (behaviors, models etc).</li> <li>Deprecate info inside <code>initialize</code> response, clients should use <code>config/updated</code> now.</li> </ul>"},{"location":"CHANGELOG/#0410","title":"0.41.0","text":"<ul> <li>Improve anthropic extraPayload requirement when adding models.</li> <li>Add message to when config failed to be parsed.</li> <li>Fix context completion for workspaces that are not git. #98</li> <li>Fix session tokens calculation.</li> </ul>"},{"location":"CHANGELOG/#0400","title":"0.40.0","text":"<ul> <li>Drop <code>agentFileRelativePath</code> in favor of behaviors customizations in the future.</li> <li>Unwrap <code>chat</code> config to be at root level.</li> <li>Fix token expiration for copilot and anthropic.</li> <li>Considerably improve toolCall approval / permissions config.</li> <li>Now with thave multiple optiosn to ask or allow tool calls, check config section.</li> </ul>"},{"location":"CHANGELOG/#0390","title":"0.39.0","text":"<ul> <li>Fix session-tokens in usage notifications.</li> <li>Support context limit on usage notifications.</li> <li>Fix session/message tokens calculation.</li> </ul>"},{"location":"CHANGELOG/#0383","title":"0.38.3","text":"<ul> <li>Fix anthropic token renew.</li> </ul>"},{"location":"CHANGELOG/#0382","title":"0.38.2","text":"<ul> <li>Fix command prompts to allow args with spaces between quotes.</li> <li>Fix anthropic token renew when expires.</li> </ul>"},{"location":"CHANGELOG/#0381","title":"0.38.1","text":"<ul> <li>Fix graalvm properties.</li> </ul>"},{"location":"CHANGELOG/#0380","title":"0.38.0","text":"<ul> <li>Improve plan-mode (prompt + eca_preview_file_change tool) #94</li> <li>Add fallback for matching / editing text in files #94</li> </ul>"},{"location":"CHANGELOG/#0370","title":"0.37.0","text":"<ul> <li>Require approval for <code>eca_shell_command</code> if running outside workspace folders.</li> <li>Fix anthropic subscription.</li> </ul>"},{"location":"CHANGELOG/#0365","title":"0.36.5","text":"<ul> <li>Fix pricing for models being case insensitive on its name when checking capabilities.</li> </ul>"},{"location":"CHANGELOG/#0364","title":"0.36.4","text":"<ul> <li>Improve api url error message when not configured.</li> </ul>"},{"location":"CHANGELOG/#0363","title":"0.36.3","text":"<ul> <li>Fix <code>anthropic/claude-3-5-haiku-20241022</code> model.</li> <li>Log json error parsing in configs.</li> </ul>"},{"location":"CHANGELOG/#0362","title":"0.36.2","text":"<ul> <li>Add login providers and server command to <code>/doctor</code>.</li> </ul>"},{"location":"CHANGELOG/#0361","title":"0.36.1","text":"<ul> <li>Improved the <code>eca_directory_tree</code> tool. #82</li> </ul>"},{"location":"CHANGELOG/#0360","title":"0.36.0","text":"<ul> <li>Support relative contexts additions via <code>~</code>, <code>./</code> <code>../</code> and <code>/</code>. #61</li> </ul>"},{"location":"CHANGELOG/#0350","title":"0.35.0","text":"<ul> <li>Anthropic subscription support, via <code>/login anthropic</code> command. #57</li> </ul>"},{"location":"CHANGELOG/#0342","title":"0.34.2","text":"<ul> <li>Fix copilot requiring login in different workspaces.</li> </ul>"},{"location":"CHANGELOG/#0341","title":"0.34.1","text":"<ul> <li>Fix proxy exception. #73</li> </ul>"},{"location":"CHANGELOG/#0340","title":"0.34.0","text":"<ul> <li>Support custom UX details/summary for MCP tools. #67</li> <li>Support clojureMCP tools diff for file changes.</li> </ul>"},{"location":"CHANGELOG/#0330","title":"0.33.0","text":"<ul> <li>Fix reasoning titles in thoughts blocks for openai-responses.</li> <li>Fix hanging LSP diagnostics requests</li> <li>Add <code>lspTimeoutSeconds</code> to config</li> <li>Support <code>HTTP_PROXY</code> and <code>HTTPS_PROXY</code> env vars for LLM request via proxies. #73</li> </ul>"},{"location":"CHANGELOG/#0324","title":"0.32.4","text":"<ul> <li>Disable <code>eca_plan_edit_file</code> in plan behavior until better idea on what plan behavior should do.</li> </ul>"},{"location":"CHANGELOG/#0323","title":"0.32.3","text":"<ul> <li>Consider <code>AGENTS.md</code> instead of <code>AGENT.md</code>, following the https://agents.md standard.</li> </ul>"},{"location":"CHANGELOG/#0322","title":"0.32.2","text":"<ul> <li>Fix option to set default chat behavior from config via <code>chat defaultBehavior</code>. #71</li> </ul>"},{"location":"CHANGELOG/#0321","title":"0.32.1","text":"<ul> <li>Fix support for models with <code>/</code> in the name like Openrouter ones.</li> </ul>"},{"location":"CHANGELOG/#0320","title":"0.32.0","text":"<ul> <li>Refactor config for better UX and understanding:</li> <li>Move <code>models</code> to inside <code>providers</code>.</li> <li>Make <code>customProviders</code> compatible with <code>providers</code>. models need to be a map now, not a list.</li> </ul>"},{"location":"CHANGELOG/#0310","title":"0.31.0","text":"<ul> <li>Update copilot models</li> <li>Drop uneeded <code>ollama useTools</code> and <code>ollama think</code> configs.</li> <li>Refactor configs for config providers unification.</li> <li><code>&lt;provider&gt;ApiKey</code> and <code>&lt;providerApiUrl&gt;</code> now live in <code>:providers \"&lt;provider&gt;\" :key</code>.</li> <li>Move <code>defaultModel</code> config from customProvider to root.</li> </ul>"},{"location":"CHANGELOG/#0300","title":"0.30.0","text":"<ul> <li>Add <code>/login</code> command to login to providers</li> <li>Add Github Copilot models support with login.</li> </ul>"},{"location":"CHANGELOG/#0292","title":"0.29.2","text":"<ul> <li>Add <code>/doctor</code> command to help with troubleshooting</li> </ul>"},{"location":"CHANGELOG/#0291","title":"0.29.1","text":"<ul> <li>Fix args streaming in toolCallPrepare to not repeat the args. https://github.com/editor-code-assistant/eca-nvim/issues/28</li> </ul>"},{"location":"CHANGELOG/#0290","title":"0.29.0","text":"<ul> <li>Add editor tools to retrieve information like diagnostics. #56</li> </ul>"},{"location":"CHANGELOG/#0280","title":"0.28.0","text":"<ul> <li>Change api for custom providers to support <code>openai-responses</code> instead of just <code>openai</code>, still supporting <code>openai</code> only.</li> <li>Add limit to repoMap with default of 800 total entries and 50 per dir. #35</li> <li>Add support for OpenAI Chat Completions API for broad third-party model support.</li> <li>A new <code>openai-chat</code> custom provider <code>api</code> type was added to support any provider using the standard OpenAI <code>/v1/chat/completions</code> endpoint.</li> <li>This enables easy integration with services like OpenRouter, Groq, DeepSeek, Together AI, and local LiteLLM instances.</li> </ul>"},{"location":"CHANGELOG/#0270","title":"0.27.0","text":"<ul> <li>Add support for auto read <code>AGENT.md</code> from workspace root and global eca dir, considering as context for chat prompts.</li> <li>Add <code>/prompt-show</code> command to show ECA prompt sent to LLM.</li> <li>Add <code>/init</code> command to ask LLM to create/update <code>AGENT.md</code> file.</li> </ul>"},{"location":"CHANGELOG/#0263","title":"0.26.3","text":"<ul> <li>breaking: Replace configs <code>ollama host</code> and <code>ollama port</code> with <code>ollamaApiUrl</code>.</li> </ul>"},{"location":"CHANGELOG/#0262","title":"0.26.2","text":"<ul> <li>Fix <code>chat/queryContext</code> to not return already added contexts</li> <li>Fix some MCP prompts that didn't work.</li> </ul>"},{"location":"CHANGELOG/#0261","title":"0.26.1","text":"<ul> <li>Fix anthropic api for custom providers.</li> <li>Support customize completion api url via custom providers.</li> </ul>"},{"location":"CHANGELOG/#0260","title":"0.26.0","text":"<ul> <li>Support manual approval for specific tools. #44</li> </ul>"},{"location":"CHANGELOG/#0250","title":"0.25.0","text":"<ul> <li>Improve plan-mode to do file changes with diffs.</li> </ul>"},{"location":"CHANGELOG/#0243","title":"0.24.3","text":"<ul> <li>Fix initializationOptions config merge.</li> <li>Fix default claude model.</li> </ul>"},{"location":"CHANGELOG/#0242","title":"0.24.2","text":"<ul> <li>Fix some commands not working.</li> </ul>"},{"location":"CHANGELOG/#0241","title":"0.24.1","text":"<ul> <li>Fix build</li> </ul>"},{"location":"CHANGELOG/#0240","title":"0.24.0","text":"<ul> <li>Get models and configs from models.dev instead of hardcoding in eca.</li> <li>Allow custom models addition via <code>models &lt;modelName&gt;</code> config.</li> <li>Add <code>/resume</code> command to resume previous chats.</li> <li>Support loading system prompts from a file.</li> <li>Fix model name parsing.</li> </ul>"},{"location":"CHANGELOG/#0231","title":"0.23.1","text":"<ul> <li>Fix openai reasoning not being included in messages.</li> </ul>"},{"location":"CHANGELOG/#0230","title":"0.23.0","text":"<ul> <li>Support parallel tool call.</li> </ul>"},{"location":"CHANGELOG/#0220","title":"0.22.0","text":"<ul> <li>Improve <code>eca_shell_command</code> to handle better error outputs.</li> <li>Add summary for eca commands via <code>summary</code> field on tool calls.</li> </ul>"},{"location":"CHANGELOG/#0211","title":"0.21.1","text":"<ul> <li>Default to gpt-5 instead of o4-mini when openai-api-key found.</li> <li>Considerably improve <code>eca_shell_command</code> to fix args parsing + git/PRs interactions.</li> </ul>"},{"location":"CHANGELOG/#0210","title":"0.21.0","text":"<ul> <li>Fix openai skip streaming response corner cases.</li> <li>Allow override payload of any LLM provider.</li> </ul>"},{"location":"CHANGELOG/#0200","title":"0.20.0","text":"<ul> <li>Support custom commands via md files in <code>~/.config/eca/commands/</code> or <code>.eca/commands/</code>.</li> </ul>"},{"location":"CHANGELOG/#0190","title":"0.19.0","text":"<ul> <li>Support <code>claude-opus-4-1</code> model.</li> <li>Support <code>gpt-5</code>, <code>gpt-5-mini</code>, <code>gpt-5-nano</code> models.</li> </ul>"},{"location":"CHANGELOG/#0180","title":"0.18.0","text":"<ul> <li>Replace <code>chat</code> behavior with <code>plan</code>.</li> </ul>"},{"location":"CHANGELOG/#0172","title":"0.17.2","text":"<ul> <li>fix query context refactor</li> </ul>"},{"location":"CHANGELOG/#0171","title":"0.17.1","text":"<ul> <li>Avoid crash MCP start if doesn't support some capabilities.</li> <li>Improve tool calling to avoid stop LLM loop if any exception happens.</li> </ul>"},{"location":"CHANGELOG/#0170","title":"0.17.0","text":"<ul> <li>Add <code>/repo-map-show</code> command. #37</li> </ul>"},{"location":"CHANGELOG/#0160","title":"0.16.0","text":"<ul> <li>Support custom system prompts via config <code>systemPromptTemplate</code>.</li> <li>Add support for file change diffs on <code>eca_edit_file</code> tool call.</li> <li>Fix response output to LLM when tool call is rejected.</li> </ul>"},{"location":"CHANGELOG/#0153","title":"0.15.3","text":"<ul> <li>Rename <code>eca_list_directory</code> to <code>eca_directory_tree</code> tool for better overview of project files/dirs.</li> </ul>"},{"location":"CHANGELOG/#0152","title":"0.15.2","text":"<ul> <li>Improve <code>eca_edit_file</code> tool for better usage from LLM.</li> </ul>"},{"location":"CHANGELOG/#0151","title":"0.15.1","text":"<ul> <li>Fix mcp tool calls.</li> <li>Improve eca filesystem calls for better tool usage from LLM.</li> <li>Fix default model selection to check anthropic api key before.</li> </ul>"},{"location":"CHANGELOG/#0150","title":"0.15.0","text":"<ul> <li>Support MCP resources as a new context.</li> </ul>"},{"location":"CHANGELOG/#0144","title":"0.14.4","text":"<ul> <li>Fix usage miscalculation.</li> </ul>"},{"location":"CHANGELOG/#0143","title":"0.14.3","text":"<ul> <li>Fix reason-id on openai models afecting chat thoughts messages.</li> <li>Support openai o models reason text when available.</li> </ul>"},{"location":"CHANGELOG/#0142","title":"0.14.2","text":"<ul> <li>Fix MCPs not starting because of graal reflection issue.</li> </ul>"},{"location":"CHANGELOG/#0141","title":"0.14.1","text":"<ul> <li>Fix native image build.</li> </ul>"},{"location":"CHANGELOG/#0140","title":"0.14.0","text":"<ul> <li>Support enable/disable tool servers.</li> <li>Bump mcp java sdk to 0.11.0.</li> </ul>"},{"location":"CHANGELOG/#0131","title":"0.13.1","text":"<ul> <li>Improve ollama model listing getting capabilities, avoiding change ollama config for different models.</li> </ul>"},{"location":"CHANGELOG/#0130","title":"0.13.0","text":"<ul> <li>Support reasoning for ollama models that support think.</li> </ul>"},{"location":"CHANGELOG/#0127","title":"0.12.7","text":"<ul> <li>Fix ollama tool calls.</li> </ul>"},{"location":"CHANGELOG/#0126","title":"0.12.6","text":"<ul> <li>fix web-search support for custom providers.</li> <li>fix output of eca_shell_command.</li> </ul>"},{"location":"CHANGELOG/#0125","title":"0.12.5","text":"<ul> <li>Improve tool call result marking as error when not expected output.</li> <li>Fix cases when tool calls output nothing.</li> </ul>"},{"location":"CHANGELOG/#0124","title":"0.12.4","text":"<ul> <li>Add chat command type.</li> </ul>"},{"location":"CHANGELOG/#0123","title":"0.12.3","text":"<ul> <li>Fix MCP prompts for anthropic models.</li> </ul>"},{"location":"CHANGELOG/#0122","title":"0.12.2","text":"<ul> <li>Fix tool calls</li> </ul>"},{"location":"CHANGELOG/#0121","title":"0.12.1","text":"<ul> <li>Improve welcome message.</li> </ul>"},{"location":"CHANGELOG/#0120","title":"0.12.0","text":"<ul> <li>Fix openai api key read from config.</li> <li>Support commands via <code>/</code>.</li> <li>Support MCP prompts via commands.</li> </ul>"},{"location":"CHANGELOG/#0112","title":"0.11.2","text":"<ul> <li>Fix error field on tool call outputs.</li> </ul>"},{"location":"CHANGELOG/#0111","title":"0.11.1","text":"<ul> <li>Fix reasoning for openai o models.</li> </ul>"},{"location":"CHANGELOG/#0110","title":"0.11.0","text":"<ul> <li>Add support for file contexts with line ranges.</li> </ul>"},{"location":"CHANGELOG/#0103","title":"0.10.3","text":"<ul> <li>Fix openai <code>max_output_tokens</code> message.</li> </ul>"},{"location":"CHANGELOG/#0102","title":"0.10.2","text":"<ul> <li>Fix usage metrics for anthropic models.</li> </ul>"},{"location":"CHANGELOG/#0101","title":"0.10.1","text":"<ul> <li>Improve <code>eca_read_file</code> tool to have better and more assertive descriptions/parameters.</li> </ul>"},{"location":"CHANGELOG/#0100","title":"0.10.0","text":"<ul> <li>Increase anthropic models maxTokens to 8196</li> <li>Support thinking/reasoning on models that support it.</li> </ul>"},{"location":"CHANGELOG/#090","title":"0.9.0","text":"<ul> <li>Include eca as a  server with tools.</li> <li>Support disable tools via config.</li> <li>Improve ECA prompt to be more precise and output with better quality</li> </ul>"},{"location":"CHANGELOG/#081","title":"0.8.1","text":"<ul> <li>Make generic tool server updates for eca native tools.</li> </ul>"},{"location":"CHANGELOG/#080","title":"0.8.0","text":"<ul> <li>Support tool call approval and configuration to manual approval.</li> <li>Initial support for repo-map context.</li> </ul>"},{"location":"CHANGELOG/#070","title":"0.7.0","text":"<ul> <li>Add client request to delete a chat.</li> </ul>"},{"location":"CHANGELOG/#061","title":"0.6.1","text":"<ul> <li>Support defaultModel in custom providers.</li> </ul>"},{"location":"CHANGELOG/#060","title":"0.6.0","text":"<ul> <li>Add usage tokens + cost to chat messages.</li> </ul>"},{"location":"CHANGELOG/#051","title":"0.5.1","text":"<ul> <li>Fix openai key</li> </ul>"},{"location":"CHANGELOG/#050","title":"0.5.0","text":"<ul> <li>Support custom LLM providers via config.</li> </ul>"},{"location":"CHANGELOG/#043","title":"0.4.3","text":"<ul> <li>Improve context query performance.</li> </ul>"},{"location":"CHANGELOG/#042","title":"0.4.2","text":"<ul> <li>Fix output of errored tool calls.</li> </ul>"},{"location":"CHANGELOG/#041","title":"0.4.1","text":"<ul> <li>Fix arguments test when preparing tool call.</li> </ul>"},{"location":"CHANGELOG/#040","title":"0.4.0","text":"<ul> <li>Add support for global rules.</li> <li>Fix origin field of tool calls.</li> <li>Allow chat communication with no workspace opened.</li> </ul>"},{"location":"CHANGELOG/#031","title":"0.3.1","text":"<ul> <li>Improve default model logic to check for configs and env vars of known models.</li> <li>Fix past messages sent to LLMs.</li> </ul>"},{"location":"CHANGELOG/#030","title":"0.3.0","text":"<ul> <li>Support stop chat prompts via <code>chat/promptStop</code> notification.</li> <li>Fix anthropic messages history.</li> </ul>"},{"location":"CHANGELOG/#020","title":"0.2.0","text":"<ul> <li>Add native tools: filesystem</li> <li>Add MCP/tool support for ollama models.</li> <li>Improve ollama integration only requiring <code>ollama serve</code> to be running.</li> <li>Improve chat history and context passed to all LLM providers.</li> <li>Add support for prompt caching for Anthropic models.</li> </ul>"},{"location":"CHANGELOG/#010","title":"0.1.0","text":"<ul> <li>Allow comments on <code>json</code> configs.</li> <li>Improve MCP tool call feedback.</li> <li>Add support for env vars in mcp configs.</li> <li>Add <code>mcp/serverUpdated</code> server notification.</li> </ul>"},{"location":"CHANGELOG/#004","title":"0.0.4","text":"<ul> <li>Add env support for MCPs</li> <li>Add web_search capability</li> <li>Add <code>o3</code> model support.</li> <li>Support custom API urls for OpenAI and Anthropic</li> <li>Add <code>--log-level &lt;level&gt;</code> option for better debugging.</li> <li>Add support for global config file.</li> <li>Improve MCP response handling.</li> <li>Improve LLM streaming response handler.</li> </ul>"},{"location":"CHANGELOG/#003","title":"0.0.3","text":"<ul> <li>Fix ollama servers discovery</li> <li>Fix <code>.eca/config.json</code> read from workspace root</li> <li>Add support for MCP servers</li> </ul>"},{"location":"CHANGELOG/#002","title":"0.0.2","text":"<ul> <li>First alpha release</li> </ul>"},{"location":"CHANGELOG/#001","title":"0.0.1","text":""},{"location":"configuration/","title":"Configuration","text":"<p>Check all available configs and its default values here.</p> <p>Also, check Examples which contains examples of users ECA configs.</p>"},{"location":"configuration/#ways-to-configure","title":"Ways to configure","text":"<p>There are multiples ways to configure ECA:</p> Global config fileLocal Config fileInitializationOptionsEnv var <p>Convenient for users and multiple projects</p> ~/.config/eca/config.json<pre><code>{\n  \"defaultBehavior\": \"plan\"\n}\n</code></pre> <p>Convenient for users</p> .eca/config.json<pre><code>{\n  \"defaultBehavior\": \"plan\"\n}\n</code></pre> <p>Convenient for editors</p> <p>Client editors can pass custom settings when sending the <code>initialize</code> request via the <code>initializationOptions</code> object:</p> <pre><code>\"initializationOptions\": {\n  \"defaultBehavior\": \"plan\"\n}\n</code></pre> <p>Via env var during server process spawn:</p> <pre><code>ECA_CONFIG='{\"myConfig\": \"my_value\"}' eca server\n</code></pre>"},{"location":"configuration/#dynamic-string-contents","title":"Dynamic string contents","text":"<p>It's possible to retrieve content of any configs with a string value using the <code>${key:value}</code> approach, being <code>key</code>:</p> <ul> <li><code>file</code>: <code>${file:/path/to/my-file}</code> or <code>${file:../rel-path/to/my-file}</code> to get a file content</li> <li><code>env</code>: <code>${env:MY_ENV}</code> to get a system env value</li> <li><code>classpath</code>: <code>${classpath:path/to/eca/file}</code> to get a file content from ECA's classpath</li> <li><code>netrc</code>: Support Unix RC credential files</li> </ul>"},{"location":"configuration/#proxy-configuration","title":"Proxy Configuration","text":"<p>ECA supports proxies with basic cleartext authentication via the de-facto env vars:</p> <pre><code>HTTP_PROXY=\"http://user:pass@host:port\"\nHTTPS_PROXY=\"http://user:pass@host:port\"\nhttp_proxy=\"http://user:pass@host:port\"\nhttps_proxy=\"http://user:pass@host:port\"\n</code></pre> <p>Lowercase var wins if both are set. Credentials (if used) must match for HTTP and HTTPS.</p>"},{"location":"configuration/#providers-models","title":"Providers / Models","text":"<p>For providers and models configuration check the dedicated models section.</p>"},{"location":"configuration/#tools","title":"Tools","text":""},{"location":"configuration/#mcp","title":"MCP","text":"<p>For MCP servers configuration, use the <code>mcpServers</code> config, examples:</p> StdioHTTP (streamable or sse) ~/.config/eca/config.json<pre><code>{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"],\n      // optional\n      \"env\": {\"FOO\": \"bar\"}\n    }\n  }\n}\n</code></pre> <p>ECA supports OAuth authentication as well</p> ~/.config/eca/config.json<pre><code>{\n  \"mcpServers\": {\n    \"cool-mcp\": {\n      \"url\": \"https://my-remote-mcp.com/mcp\"\n    }\n  }\n}\n</code></pre>"},{"location":"configuration/#approval-permissions","title":"Approval / permissions","text":"<p>By default, ECA asks to call any non read-only tool (default here), but that can easily be configured in several ways via the <code>toolCall.approval</code> config.</p> <p>You can configure the default behavior with <code>byDefault</code> and/or configure a tool in <code>ask</code>, <code>allow</code> or <code>deny</code> configs.</p> <p>Check some examples:</p> Allow any tools by defaultAllow all but some toolsAsk all but all tools from some mcpsMatching by a tool argumentDenying a tool ~/.config/eca/config.json<pre><code>{\n  \"toolCall\": {\n    \"approval\": {\n      \"byDefault\": \"allow\"\n    }\n  }\n}\n</code></pre> ~/.config/eca/config.json<pre><code>{\n  \"toolCall\": {\n    \"approval\": {\n      \"byDefault\": \"allow\",\n      \"ask\": {\n        \"eca_editfile\": {},\n        \"my-mcp__my_tool\": {}\n      }\n    }\n  }\n}\n</code></pre> ~/.config/eca/config.json<pre><code>{\n  \"toolCall\": {\n    \"approval\": {\n      // \"byDefault\": \"ask\", not needed as it's eca default\n      \"allow\": {\n        \"eca\": {},\n        \"my-mcp\": {}\n      }\n    }\n  }\n}\n</code></pre> <p><code>argsMatchers</code> is a map of argument name by list of java regex.</p> ~/.config/eca/config.json<pre><code>{\n  \"toolCall\": {\n    \"approval\": {\n      \"byDefault\": \"allow\",\n      \"ask\": {\n        \"shell_command\": {\"argsMatchers\": {\"command\": [\".*rm.*\",\n                                                           \".*mv.*\"]}}\n      }\n    }\n  }\n}\n</code></pre> ~/.config/eca/config.json<pre><code>{\n  \"toolCall\": {\n    \"approval\": {\n      \"byDefault\": \"allow\",\n      \"deny\": {\n        \"shell_command\": {\"argsMatchers\": {\"command\": [\".*rm.*\",\n                                                       \".*mv.*\"]}}\n      }\n    }\n  }\n}\n</code></pre> <p>Also check the <code>plan</code> behavior which is safer.</p> <p>The <code>manualApproval</code> setting was deprecated and replaced by the <code>approval</code> one without breaking changes</p>"},{"location":"configuration/#file-reading","title":"File Reading","text":"<p>You can configure the maximum number of lines returned by the <code>eca__read_file</code> tool:</p> ~/.config/eca/config.json<pre><code>{\n  \"toolCall\": {\n    \"readFile\": {\n      \"maxLines\": 1000\n    }\n  }\n}\n</code></pre> <p>Default: <code>2000</code> lines</p>"},{"location":"configuration/#custom-tools","title":"Custom Tools","text":"<p>You can define your own command-line tools that the LLM can use. These are configured via the <code>customTools</code> key in your <code>config.json</code>.</p> <p>The <code>customTools</code> value is an object where each key is the name of your tool. Each tool definition has the following properties:</p> <ul> <li><code>description</code>: A clear description of what the tool does. This is crucial for the LLM to decide when to use it.</li> <li><code>command</code>: An string representing the command and its static arguments.</li> <li><code>schema</code>: An object that defines the parameters the LLM can provide.<ul> <li><code>properties</code>: An object where each key is an argument name.</li> <li><code>required</code>: An array of required argument names.</li> </ul> </li> </ul> <p>Placeholders in the format <code>{{argument_name}}</code> within the <code>command</code> string will be replaced by the values provided by the LLM.</p> Example 1Example 2 ~/.config/eca/config.json<pre><code>{\n  \"customTools\": {\n    \"web-search\": {\n      \"description\": \"Fetches the content of a URL and returns it in Markdown format.\",\n      \"command\": \"trafilatura --output-format=markdown -u {{url}}\",\n      \"schema\": {\n        \"properties\": {\n          \"url\": {\n            \"type\": \"string\",\n            \"description\": \"The URL to fetch content from.\"\n          }\n        },\n        \"required\": [\"url\"]\n      }\n    }\n  }\n}\n</code></pre> ~/.config/eca/config.json<pre><code>{\n  \"customTools\": {\n    \"file-search\": {\n      \"description\": \"${file:tools/my-tool.md}\",\n      \"command\": \"find {{directory}} -name {{pattern}}\",\n      \"schema\": {\n        \"properties\": {\n          \"directory\": {\n            \"type\": \"string\",\n            \"description\": \"The directory to start the search from.\"\n          },\n          \"pattern\": {\n            \"type\": \"string\",\n            \"description\": \"The search pattern for the filename (e.g., '*.clj').\"\n          }\n        },\n        \"required\": [\"directory\", \"pattern\"]\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"configuration/#skills","title":"Skills","text":"<p>Skills are folders with <code>SKILL.md</code> which teachs LLM how to solve a specific task or gain knowledge about it. Following the agentskills standard, ECA search for skills following <code>~/.config/eca/skills/some-skill/SKILL.md</code> and <code>.eca/skills/some-skill/SKILL.md</code> which should contain <code>name</code> and <code>description</code> metadatas.</p> <p>When sending a prompt request to LLM, ECA will send only name and description of all available skills, LLM then can choose to load a skill via <code>eca__skill</code> tool if that matches user request. </p> <p>Check the examples:</p> Simple lint skillMore complex skill using scriptsEnable/disable specific skills ~/.config/eca/skills/lint-fix/SKILL.md<pre><code>---\nname: lint-fix\ndescription: Learn how to lint and fix the code\n---\n\n# Instructions\n\nRun `clojure-lsp diagnostics` to lint the code\n</code></pre> ~/.config/eca/skills/gif-generator/SKILL.md<pre><code>---\nname: gif-generator\ndescription: Knowledge and utils to create gifs. Provide concepts and scripts, use when requested to create gifs.\n---\n\n- Use scripts/gif-generate.py passing gif name and dimensions.\n- &lt;More complex instructions here&gt;\n...\n</code></pre> ~/.config/eca/skills/gif-generator/scripts/generator.py<pre><code>from PIL import Image\n# Python code that generates a gif here\n....\n</code></pre> <p>It's possible to control which skills LLM have access globally or for a specific behavior. You just need to define a tool call approval for the <code>eca__skill</code> for a specific skill <code>name</code>:</p> <p>Example disabling all skills but one for a behavior</p> ~/.config/eca/config.json<pre><code>{\n  \"toolCall\": {\n    \"approval\": {\n      \"deny\": {\n        \"eca__skill\": {}\n      }\n    }\n  },\n  \"behavior\": {\n    \"reviewer\": {\n      \"toolCall\": {\n        \"approval\": {\n          \"allow\": {\n            \"eca__skill\": {\"argsMatchers\": {\"name\": [\"my-skill\"]}}\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>You can have more directories and contents like <code>scripts/</code>, <code>references/</code>, <code>assets/</code> for a skill making it really powerful, check the spec for more details.</p>"},{"location":"configuration/#rules","title":"Rules","text":"<p>Rules are contexts that are passed to the LLM during a prompt and are useful to tune prompts or LLM behavior. Rules are text files (typically <code>.md</code>, but any format works):</p> <p>There are 3 possible ways to configure rules following this order of priority:</p> Project fileGlobal fileConfig <p>A <code>.eca/rules</code> folder from the workspace root containing <code>.md</code> files with the rules.</p> .eca/rules/talk_funny.md<pre><code>- Talk funny like Mickey!\n</code></pre> <p>A <code>$XDG_CONFIG_HOME/eca/rules</code> or <code>~/.config/eca/rules</code> folder containing <code>.md</code> files with the rules.</p> ~/.config/eca/rules/talk_funny.md<pre><code>- Talk funny like Mickey!\n</code></pre> <p>Just add toyour config the <code>:rules</code> pointing to <code>.md</code> files that will be searched from the workspace root if not an absolute path:</p> ~/.config/eca/config.json<pre><code>{\n  \"rules\": [{\"path\": \"my-rule.md\"}]\n}\n</code></pre>"},{"location":"configuration/#custom-command-prompts","title":"Custom command prompts","text":"<p>You can configure custom command prompts for project, global or via <code>commands</code> config pointing to the path of the commands. Prompts can use variables like <code>$ARGUMENTS</code>, <code>$ARG1</code>, <code>ARG2</code>, to replace in the prompt during command call.</p> <p>You can configure in multiple different ways:</p> Local custom commandsGlobal custom commandsConfig <p>A <code>.eca/commands</code> folder from the workspace root containing <code>.md</code> files with the custom prompt.</p> .eca/commands/check-performance.md<pre><code>Check for performance issues in $ARG1 and optimize if needed.\n</code></pre> <p>ECA will make available a <code>/check-performance</code> command after creating that file.</p> <p>A <code>$XDG_CONFIG_HOME/eca/commands</code> or <code>~/.config/eca/commands</code> folder containing <code>.md</code> files with the custom command prompt.</p> ~/.config/eca/commands/check-performance.md<pre><code>Check for performance issues in $ARG1 and optimize if needed.\n</code></pre> <p>ECA will make available a <code>/check-performance</code> command after creating that file.</p> <p>Just add to your config the <code>commands</code> pointing to <code>.md</code> files that will be searched from the workspace root if not an absolute path:</p> ~/.config/eca/config.json<pre><code>{\n  \"commands\": [{\"path\": \"my-custom-prompt.md\"}]\n}\n</code></pre> <p>ECA will make available a <code>/my-custom-prompt</code> command after creating that file.</p>"},{"location":"configuration/#behaviors-prompts","title":"Behaviors / prompts","text":"<p>ECA allows to totally customize the prompt sent to LLM via the <code>behavior</code> config, allowing to have multiple behaviors for different tasks or workflows.</p> Example: my-behavior ~/.config/eca/config.json<pre><code>{\n  \"behavior\": {\n    \"my-behavior\": {\n      \"systemPrompt\": \"${file:/path/to/my-behavior-prompt.md}\"\n    }\n  }\n}\n</code></pre>"},{"location":"configuration/#hooks","title":"Hooks","text":"<p>Hooks are shell actions that run before or after specific events, useful for notifications, injecting context, modifying inputs, or blocking tool calls.</p>"},{"location":"configuration/#hook-types","title":"Hook Types","text":"Type When Can Modify <code>sessionStart</code> Server initialized - <code>sessionEnd</code> Server shutting down - <code>chatStart</code> New chat or resumed chat Can inject <code>additionalContext</code> <code>chatEnd</code> Chat deleted - <code>preRequest</code> Before prompt sent to LLM Can rewrite prompt, inject context, stop request <code>postRequest</code> After prompt finished - <code>preToolCall</code> Before tool execution Can modify args, override approval, reject <code>postToolCall</code> After tool execution Can inject context for next LLM turn"},{"location":"configuration/#hook-options","title":"Hook Options","text":"<ul> <li><code>matcher</code>: Regex for <code>server__tool-name</code>, only for <code>*ToolCall</code> hooks.</li> <li><code>visible</code>: Show hook execution in chat (default: <code>true</code>).</li> <li><code>runOnError</code>: For <code>postToolCall</code>, run even if tool errored (default: <code>false</code>).</li> </ul>"},{"location":"configuration/#execution-details","title":"Execution Details","text":"<ul> <li>Order: Alphabetical by key. Prompt rewrites chain; argument updates merge (last wins).</li> <li>Conflict: Any rejection (<code>deny</code> or exit <code>2</code>) blocks the call immediately.</li> <li>Timeout: Actions time out after 30s unless <code>\"timeout\": ms</code> is set.</li> </ul>"},{"location":"configuration/#input-output","title":"Input / Output","text":"<p>Hooks receive JSON via stdin with event data (top-level keys <code>snake_case</code>, nested data preserves case). Common fields:</p> <ul> <li>All hooks: <code>hook_name</code>, <code>hook_type</code>, <code>workspaces</code>, <code>db_cache_path</code></li> <li>Chat hooks add: <code>chat_id</code>, <code>behavior</code></li> <li>Tool hooks add: <code>tool_name</code>, <code>server</code>, <code>tool_input</code>, <code>approval</code> (pre) or <code>tool_response</code>, <code>error</code> (post)</li> <li><code>chatStart</code> adds: <code>resumed</code> (boolean)</li> </ul> <p>Hooks can output JSON to control behavior:</p> <pre><code>{\n  \"additionalContext\": \"Extra context for LLM\",  // injected as XML block\n  \"replacedPrompt\": \"New prompt text\",           // preRequest only\n  \"updatedInput\": {\"key\": \"value\"},              // preToolCall: merge into tool args\n  \"approval\": \"allow\" | \"ask\" | \"deny\",          // preToolCall: override approval\n  \"continue\": false,                             // stop processing (with optional stopReason)\n  \"stopReason\": \"Why stopped\",\n  \"suppressOutput\": true                         // hide hook output from chat\n}\n</code></pre> <p>Plain text output (non-JSON) is treated as <code>additionalContext</code>.</p> <p>To reject a tool call, either output <code>{\"approval\": \"deny\"}</code> or exit with code <code>2</code>.</p>"},{"location":"configuration/#examples","title":"Examples","text":"Notify after promptRing bell sound when pending tool call approvalInject context on chat startRewrite promptBlock tool with JSON responseModify tool argumentsUse external script file ~/.config/eca/config.json<pre><code>{\n  \"hooks\": {\n    \"notify-me\": {\n      \"type\": \"postRequest\",\n      \"visible\": false,\n      \"actions\": [{\"type\": \"shell\", \"shell\": \"notify-send 'Prompt finished!'\"}]\n    }\n  }\n}\n</code></pre> ~/.config/eca/hooks/my-hook.sh<pre><code>jq -e '.approval == \"ask\"' &gt; /dev/null &amp;&amp; canberra-gtk-play -i complete\n</code></pre> ~/.config/eca/config.json<pre><code>{\n  \"hooks\": {\n    \"notify-me\": {\n      \"type\": \"preToolCall\",\n      \"visible\": false,\n      \"actions\": [\n        {\n          \"type\": \"shell\",\n          \"file\": \"hooks/my-hook.sh\"\n        }\n      ]\n    }\n  }\n}\n</code></pre> ~/.config/eca/config.json<pre><code>{\n  \"hooks\": {\n    \"load-context\": {\n      \"type\": \"chatStart\",\n      \"actions\": [{\n        \"type\": \"shell\",\n        \"shell\": \"echo '{\\\"additionalContext\\\": \\\"Today is '$(date +%Y-%m-%d)'\\\"}'\"\n      }]\n    }\n  }\n}\n</code></pre> ~/.config/eca/config.json<pre><code>{\n  \"hooks\": {\n    \"add-prefix\": {\n      \"type\": \"preRequest\",\n      \"actions\": [{\n        \"type\": \"shell\",\n        \"shell\": \"jq -c '{replacedPrompt: (\\\"[IMPORTANT] \\\" + .prompt)}'\"\n      }]\n    }\n  }\n}\n</code></pre> ~/.config/eca/config.json<pre><code>{\n  \"hooks\": {\n    \"block-rm\": {\n      \"type\": \"preToolCall\",\n      \"matcher\": \"eca__shell_command\",\n      \"actions\": [{\n        \"type\": \"shell\",\n        \"shell\": \"if jq -e '.tool_input.command | test(\\\"rm -rf\\\")' &gt; /dev/null; then echo '{\\\"approval\\\":\\\"deny\\\",\\\"additionalContext\\\":\\\"Dangerous command blocked\\\"}'; fi\"\n      }]\n    }\n  }\n}\n</code></pre> ~/.config/eca/config.json<pre><code>{\n  \"hooks\": {\n    \"force-recursive\": {\n      \"type\": \"preToolCall\",\n      \"matcher\": \"eca__directory_tree\",\n      \"actions\": [{\n        \"type\": \"shell\",\n        \"shell\": \"echo '{\\\"updatedInput\\\": {\\\"max_depth\\\": 3}}'\"\n      }]\n    }\n  }\n}\n</code></pre> ~/.config/eca/config.json<pre><code>{\n  \"hooks\": {\n    \"my-hook\": {\n      \"type\": \"preToolCall\",\n      \"actions\": [{\"type\": \"shell\", \"file\": \"~/.config/eca/hooks/check-tool.sh\"}]\n    }\n  }\n}\n</code></pre>"},{"location":"configuration/#completion","title":"Completion","text":"<p>You can configure which model and system prompt ECA will use during its inline completion:</p> Example ~/.config/eca/config.json<pre><code>{\n  \"completion\": {\n    \"model\": \"github-copilot/gpt-4.1\",\n    \"systemPrompt\": \"${file:/path/to/my-prompt.md}\"\n  }\n}\n</code></pre>"},{"location":"configuration/#rewrite","title":"Rewrite","text":"<p>Configure the model and system prompt used for ECA's rewrite feature via the <code>rewrite</code> config. By default, ECA follows the same model selection as chat unless overwritten:</p> Example ~/.config/eca/config.json<pre><code>{\n  \"rewrite\": {\n    \"model\": \"github-copilot/gpt-4.1\",\n    \"systemPrompt\": \"${file:/path/to/my-prompt.md}\"\n  }\n}\n</code></pre>"},{"location":"configuration/#opentelemetry-integration","title":"Opentelemetry integration","text":"<p>To configure, add your OTLP collector config via <code>:otlp</code> map following otlp auto-configure settings. Example:</p> ~/.config/eca/config.json<pre><code>{\n  \"otlp\": {\n    \"otel.exporter.otlp.metrics.protocol\": \"http/protobuf\",\n    \"otel.exporter.otlp.metrics.endpoint\": \"https://my-otlp-endpoint.com/foo\",\n    \"otel.exporter.otlp.headers\": \"Authorization=Bearer 123456\"\n  }\n}\n</code></pre>"},{"location":"configuration/#all-configs","title":"All configs","text":"SchemaDefault values <pre><code>interface Config {\n    providers?: {[key: string]: {\n        api?: 'openai-responses' | 'openai-chat' | 'anthropic';\n        fetchModels?: boolean;\n        url?: string;\n        key?: string; // when provider supports api key.\n        keyRc?: string; // credential file lookup in format [login@]machine[:port]\n        completionUrlRelativePath?: string;\n        thinkTagStart?: string;\n        thinkTagEnd?: string;\n        models: {[key: string]: {\n          modelName?: string;\n          extraPayload?: {[key: string]: any}\n        }};\n    }};\n    defaultModel?: string;\n    hooks?: {[key: string]: {\n            type: 'sessionStart' | 'sessionEnd' | 'chatStart' | 'chatEnd' |\n                  'preRequest' | 'postRequest' | 'preToolCall' | 'postToolCall';\n            matcher?: string; // regex for server__tool-name, only *ToolCall hooks\n            visible?: boolean;\n            runOnError?: boolean; // postToolCall only\n            actions: {\n                type: 'shell';\n                shell?: string; // inline script\n                file?: string;  // path to script file\n                timeout?: number; // ms, default 30000\n            }[];\n        };\n    };\n    rules?: [{path: string;}];\n    enabledSkills?: string[];\n    disabledTools?: string[],\n    commands?: [{path: string;}];\n    behavior?: {[key: string]: {\n        systemPrompt?: string;\n        defaultModel?: string;\n        disabledTools?: string[];\n        enabledSkills?: string[];\n        toolCall?: {\n            approval?: {\n                byDefault?: 'ask' | 'allow' | 'deny';\n                allow?: {[key: string]: {argsMatchers?: {[key: string]: string[]}}};\n                ask?: {[key: string]: {argsMatchers?: {[key: string]: string[]}}};\n                deny?: {[key: string]: {argsMatchers?: {[key: string]: string[]}}};\n            };\n        };\n    }};\n    customTools?: {[key: string]: {\n        description: string;\n        command: string;\n        schema: {\n            properties: {[key: string]: {\n                type: string;\n                description: string;\n            }};\n            required: string[];\n        };\n    }};\n    toolCall?: {\n      approval?: {\n        byDefault: 'ask' | 'allow';\n        allow?: {{key: string}: {argsMatchers?: {{[key]: string}: string[]}}},\n        ask?: {{key: string}: {argsMatchers?: {{[key]: string}: string[]}}},\n        deny?: {{key: string}: {argsMatchers?: {{[key]: string}: string[]}}},\n      };\n      readFile?: {\n        maxLines?: number;\n      };\n      shellCommand?: {\n        summaryMaxLength?: number,\n      };\n    };\n    mcpTimeoutSeconds?: number;\n    lspTimeoutSeconds?: number;\n    mcpServers?: {[key: string]: {\n        url?: string; // for remote http-stremable servers\n        command?: string; // for stdio servers\n        args?: string[];\n        disabled?: boolean;\n    }};\n    defaultBehavior?: string;\n    welcomeMessage?: string;\n    compactPromptFile?: string;\n    index?: {\n        ignoreFiles: [{\n            type: string;\n        }];\n        repoMap?: {\n            maxTotalEntries?: number;\n            maxEntriesPerDir?: number;\n        };\n    };\n    completion?: {\n        model?: string;\n        systemPrompt?: string;\n    };\n    rewrite?: {\n        model?: string;\n        systemPrompt?: string;\n    };\n    otlp?: {[key: string]: string};\n    netrcFile?: string;\n}\n</code></pre> <pre><code>{\n  \"providers\": {\n      \"openai\": {\"url\": \"https://api.openai.com\"},\n      \"anthropic\": {\"url\": \"https://api.anthropic.com\"},\n      \"github-copilot\": {\"url\": \"https://api.githubcopilot.com\"},\n      \"google\": {\"url\": \"https://generativelanguage.googleapis.com/v1beta/openai\"},\n      \"ollama\": {\"url\": \"http://localhost:11434\"}\n  },\n  \"defaultModel\": null, // let ECA decides the default model.\n  \"netrcFile\": null, // search ~/.netrc or ~/_netrc when null.\n  \"hooks\": {},\n  \"rules\" : [],\n  \"commands\" : [],\n  \"enabledSkills\": [\".*\"],\n  \"disabledTools\": [],\n  \"toolCall\": {\n    \"approval\": {\n      \"byDefault\": \"ask\",\n      \"allow\": {\"eca__directory_tree\": {},\n                \"eca__read_file\": {},\n                \"eca__grep\": {},\n                \"eca__preview_file_change\": {},\n                \"eca__editor_diagnostics\": {}},\n      \"ask\": {},\n      \"deny\": {}\n    },\n    \"readFile\": {\n      \"maxLines\": 2000\n    },\n    \"shellCommand\": {\n      \"summaryMaxLength\": 30,\n    },\n  },\n  \"mcpTimeoutSeconds\" : 60,\n  \"lspTimeoutSeconds\" : 30,\n  \"mcpServers\" : {},\n  \"behavior\" {\n    \"agent\": {\"systemPrompt\": \"${classpath:prompts/agent_behavior.md}\",\n              \"disabledTools\": [\"preview_file_change\"]},\n    \"plan\": {\"systemPrompt\": \"${classpath:prompts/plan_behavior.md}\",\n              \"disabledTools\": [\"edit_file\", \"write_file\", \"move_file\"],\n              \"toolCall\": {\"approval\": {\"deny\": {\"eca__shell_command\":\n                                                 {\"argsMatchers\": {\"command\" [\".*&gt;.*\",\n                                                                              \".*\\\\|\\\\s*(tee|dd|xargs).*\",\n                                                                              \".*\\\\b(sed|awk|perl)\\\\s+.*-i.*\",\n                                                                              \".*\\\\b(rm|mv|cp|touch|mkdir)\\\\b.*\",\n                                                                              \".*git\\\\s+(add|commit|push).*\",\n                                                                              \".*npm\\\\s+install.*\",\n                                                                              \".*-c\\\\s+[\\\"'].*open.*[\\\"']w[\\\"'].*\",\n                                                                              \".*bash.*-c.*&gt;.*\"]}}}}}}\n  }\n  \"defaultBehavior\": \"agent\",\n  \"welcomeMessage\" : \"Welcome to ECA!\\n\\nType '/' for commands\\n\\n\",\n  \"compactPromptFile\": \"prompts/compact.md\",\n  \"index\" : {\n    \"ignoreFiles\" : [ {\n      \"type\" : \"gitignore\"\n    } ],\n    \"repoMap\": {\n      \"maxTotalEntries\": 800,\n      \"maxEntriesPerDir\": 50\n    }\n  },\n  \"completion\": {\n    \"model\": \"openai/gpt-4.1\",\n    \"systemPrompt\": \"${classpath:prompts/inline_completion.md}\"\n  },\n  \"rewrite\": {\n    \"systemPrompt\": \"${classpath:prompts/rewrite.md}\"\n  }\n}\n</code></pre>"},{"location":"development/","title":"ECA Development","text":""},{"location":"development/#building-local","title":"Building local","text":"<ul> <li>Install babashka.</li> <li>Run <code>bb debug-cli</code>, it will generate a JVM embedded binary at project root where you can just <code>./eca</code>.</li> </ul>"},{"location":"development/#project-structure","title":"Project structure","text":"<p>The ECA codebase follows a pragmatic layered layout that separates concerns clearly so that you can jump straight to the part you need to change.</p>"},{"location":"development/#files-overview","title":"Files overview","text":"Path Responsibility <code>bb.edn</code> Babashka tasks (e.g. <code>bb test</code>, <code>bb debug-cli</code>) for local workflows and CI, the main entrypoint for most tasks. <code>deps.edn</code> Clojure dependency coordinates and aliases used by the JVM build and the native GraalVM image. <code>docs/</code> Markdown documentation shown at https://eca.dev <code>src/eca/config.clj</code> Centralized place to get ECA configs from multiple places. <code>src/eca/logger.clj</code> Logger interface to log to stderr. <code>src/eca/shared.clj</code> shared utility fns to whole project. <code>src/eca/db.clj</code> Simple in-memory KV store that backs sessions/MCP, all in-memory statue lives here. <code>src/eca/llm_api.clj</code> Public fa\u00e7ade used by features to call an LLM. <code>src/eca/llm_providers/</code> Vendor adapters (<code>openai.clj</code>, <code>anthropic.clj</code>, <code>ollama.clj</code>). <code>src/eca/llm_util.clj</code> Token counting, chunking, rate-limit helpers. <code>src/eca/features/</code> High-level capabilities exposed to the editor \u251c\u2500 <code>chat.clj</code> Streaming chat orchestration &amp; tool-call pipeline. \u251c\u2500 <code>prompt.clj</code> Prompt templates and variable interpolation. \u251c\u2500 <code>index.clj</code> Embedding &amp; retrieval-augmented generation helpers. \u251c\u2500 <code>rules.clj</code> Guards that enforce user-defined project rules. \u251c\u2500 <code>tools.clj</code> Registry of built-in tool descriptors (run, approve\u2026). \u2514\u2500 <code>tools/</code> Implementation of side-effectful tools: \u2500\u2500\u251c\u2500 <code>filesystem.clj</code> read/write/edit helpers\u2003 \u2500\u2500\u251c\u2500 <code>shell.clj</code> runs user-approved shell commands\u2003 \u2500\u2500\u251c\u2500 <code>mcp.clj</code> Multi-Command Plan supervisor\u2003 \u2500\u2500\u2514\u2500 <code>util.clj</code> misc helpers shared by tools. <code>src/eca/messenger.clj</code> To send back to client requests/notifications over stdio. <code>src/eca/handlers.clj</code> Entrypoint for all features. <code>src/eca/server.clj</code> stdio entry point; wires everything together via <code>jsonrpc4clj</code>. <code>src/eca/main.clj</code> The CLI interface. <code>src/eca/nrepl.clj</code> Starts an nREPL when <code>:debug</code> flag is passed. <p>Together these files implement the request flow:</p> <p><code>client/editor</code> \u2192 <code>stdin JSON-RPC</code> \u2192 <code>handlers</code> \u2192 <code>features</code> \u2192 <code>llm_api</code> \u2192 <code>llm_provider</code> \u2192 results streamed back.</p> <p>With this map you can usually answer:</p> <ul> <li>\"Where does request X enter the system?\" \u2013 look in <code>handlers.clj</code>.</li> <li>\"How is tool Y executed?\" \u2013 see <code>src/eca/features/tools/&lt;y&gt;.clj</code>.</li> <li>\"How do we talk to provider Z?\" \u2013 adapter under <code>llm_providers/</code>.</li> </ul>"},{"location":"development/#unit-tests","title":"Unit Tests","text":"<p>Run with <code>bb test</code> or run test via Clojure REPL. CI will run the same task.</p>"},{"location":"development/#integration-tests","title":"Integration Tests","text":"<p>Run with <code>bb integration-test</code>, it will use your <code>eca</code> binary project root to spawn a server process and communicate with it via JSONRPC, testing the whole eca flow like an editor.</p>"},{"location":"development/#coding","title":"Coding","text":"<p>There are several ways of finding and fixing a bug or implementing a new feature:</p> <ul> <li>Create a test for your bug/feature, then implement the code following the test (TDD).</li> <li>Build a local <code>eca</code> JVM embedded binary using <code>bb debug-cli</code> (requires <code>babashka</code>), and test it manually in your client pointing to it. After started, you can connect to the nrepl port mentioned in eca stderr, do you changes, evaluate and it will be affected on the running eca.</li> <li>Using a debug binary you can connect to the REPL, make changes to the running eca process (really handy).</li> </ul>"},{"location":"development/#supporting-a-new-editor","title":"Supporting a new editor","text":"<p>When supporting a new editor, it's important to keep UX consistency across editors, check how other editors done or ask for help.</p> <p>This step-by-step feature implementation help track progress and next steps:</p> <pre><code>- [ ] Create the plugin/extension repo (editor-code-assistant/eca-&lt;editor&gt; would be ideal), ask maintainers for permission.\n- Server\n  - Manage ECA server process.\n    - [ ] Automatic download of latest server.\n    - [ ] Allow user specify server path/args.\n    - [ ] Commands for Start/stop server from editor.\n    - [ ] Show server status (modeline, bottom of editor, etc).\n  - [ ] JSONRPC communication with eca server process via stdin/stdout sending/receiving requests and notifications, check [protocol](./protocol.md).\n  - [ ] Allow check eca server process stderr for debugging/logs.\n  - [ ] Support `initialize` and `initialized` methods.\n  - [ ] Support `exit` and `shutdown` methods.\n- Chat\n  - [ ] Open chat window\n  - [ ] Send user messages via `chat/prompt` request.\n  - [ ] Clear chat and Reset chat.\n  - [ ] Support receive chat contents via `chat/contentReceived` notification.\n  - [ ] Present and allow user change behaviors and models returned from `initialize` request.\n  - [ ] Present and add contexts via `chat/queryContext` request\n  - [ ] Support tools contents: run/approval/reject via `chat/toolCallApprove` or `chat/toolCallReject`.\n  - [ ] Support tools details: showing a file change like a diff.\n  - [ ] Support reason/thoughts content blocks.\n  - [ ] Show MCPs summary (running, failed, pending).\n  - [ ] Support chat commands (`/`) auto completion, querying via `chat/queryCommands`.\n  - [ ] Show usage (costs/tokens) from usage content blocks.\n  - [ ] keybindings: navigate through chat blocks/messages, clear chat.\n- MCP\n  - [ ] Open MCP details window\n  - [ ] Receive MCP server updates and update chat and mcp-details ux.\n- [ ] Basic plugin/extension documentation\n</code></pre> <p>Create a issue to help track the effort copying and pasting these check box to help track progress, example.</p> <p>Please provide feedback of the difficulties implementing your server, especially missing docs, to make next integrations smoother!</p>"},{"location":"examples/","title":"Examples","text":"<p>ECA config examples showing the power of its features and flexibility</p> <p>If you think your config is relevant to be shared for other people, open a pull request here</p>"},{"location":"examples/#from-users","title":"From users","text":"Hook: fix unbalanced CLJ parens (@zikajk) <p>First install latest [babashka] + [bbin]. Then run: <code>bbin install https://github.com/bhauman/clojure-mcp-light.git --as clj-paren-repair --main-opts '[\"-m\"  \"clojure-mcp-light.paren-repair\"]'</code></p> config.json<pre><code>{...\n \"hooks: {\"CLJ-balanced-parens-check\": {\"type\":\"postToolCall\",\n                                        \"matcher\": \"eca__write_file|eca__edit_file\",\n                                        \"actions\": [{\"type\": \"shell\",\n                                                     \"file\": \"hooks/clj_check_parens.sh\"}]}}\n...}\n</code></pre> hooks/clj_check_parens.sh<pre><code># Hook to check Clojure files with clj-kondo and auto-repair parens\n\n# Read stdin and extract path (returns empty string if null/invalid)\nfile_path=$(jq -r '.tool_input.path // empty' 2&gt;/dev/null)\n\n# Helper function to generate JSON output\nrespond() {\n  cat &lt;&lt;EOF\n{\n  \"suppressOutput\": $1,\n  \"systemMessage\": \"$2\",\n  \"additionalContext\": \"$3\"\n}\nEOF\n}\n\n# 1. Guard Clause: Exit if no path or not a Clojure file\nif [[ -z \"$file_path\" || ! \"$file_path\" =~ \\.(clj|cljs|cljd|cljc)$ ]]; then\n  respond true\n  exit 0\nfi\n\n# 2. Run clj-kondo (We only care about Exit Code 3: Unbalanced Parens)\nclj-kondo --lint \"$file_path\" &amp;&gt;/dev/null\nif [ $? -ne 3 ]; then\n  respond true\n  exit 0\nfi\n\n# 3. Attempt Repair\nif clj-paren-repair \"$file_path\" &amp;&gt;/dev/null; then\n  respond false \"Unbalanced parens fixed.\" \"Unbalanced parens have been automatically fixed.\"\nelse\n  respond false \"Unbalanced parens not fixed!\" \"Unbalanced parens couldn't be automatically fixed. Tell user to fix it manually.\"\nfi\n\nexit 0\n</code></pre> Custom behavior: Clojure reviewer (@zikajk) config.json<pre><code>{\n    \"behavior\": {\n        \"reviewer\": {\n            \"defaultModel\": \"openai/gpt-5.1\",\n            \"systemPrompt\": \"${file:prompts/reviewer.md}\"}\n        },\n        \"dangerous\": {\n            \"defaultModel\": \"deepseek/deepseek-chat\",\n            \"toolCall\": {\"approval\": {\"byDefault\": \"allow\"}}\n        }\n\n}\n</code></pre> prompts/reviwer.md<pre><code>&lt;personality&gt;\nYou are a Principal Clojure(Script) Engineer, acting as a wise and pragmatic code reviewer. Your mindset is shaped by the design principles of Rich Hickey and the practical wisdom found in texts like \"Elements of Clojure,\" \"Functional Design,\" and \"Programming Clojure.\" Your tone is constructive; your goal is to help, not just to criticize.\n&lt;/personality&gt;\n\n&lt;goal&gt;\nReview the following staged changes, which are part of a large, monolithic codebase. Your goal is not just to find errors, but to elevate the code's design, maintainability, and simplicity.\n\nDeliver production-quality solutions that meet the stated scope, nothing more and nothing less. Prefer clarity, simplicity, and testability over cleverness. Design for change. Always apply the **Boy Scout Rule**: leave the code a little cleaner than you found it.\n&lt;/goal&gt;\n\n&lt;instructions&gt;\nYour review must be concise and provide actionable feedback. Focus on the following key areas, adhering to these hard rules.\n\n### 1. Structure and Size (Measurable Rules)\n- **Nesting and Complexity:** Look for deeply nested structures (`let`, `if`, `cond`). If the code requires more than 2-3 levels of nesting, it's a signal to refactor. Suggest extracting logic into separate functions.\n- **No Magic Values:** Are there \"magic\" numbers or strings in the code? Suggest replacing them with named constants (`def` or `defconst`).\n\n### 2. State Management and Side Effects\n- **Purity:** Prefer pure functions. Are side effects (I/O, database, time, randomness) clearly separated from the core logic?\n- **Explicit Side Effects:** Are functions with side effects clearly named (e.g., with a `!` suffix)? Are these effects contained at the system's boundaries?\n- **Correct Atom Usage:** Is an `atom` used for simple, uncoordinated state? Is there complex logic hidden within it that deserves a better model (e.g., a state machine)?\n\n### 3. Idiomatic Clojure &amp; Code Smells\n- **Idiomatic Core Usage:** Does the code make full use of `clojure.core` functions (e.g., `update`, `get-in`, sequence functions) instead of manual re-implementations?\n- **Duplication (DRY):** Identify any copied code block (approx. **5+ lines**) and suggest extracting it into a reusable function.\n- **Primitive Obsession:** Does the code work with too many simple types (strings, numbers) where a structured data type would make more sense? Suggest using `defrecord` or validation with `clojure.spec`/`malli` to create small \"value objects.\"\n- **Error Handling:** Is error handling robust? Prefer exceptions with rich context (`ex-info`) over returning `nil` for control flow, unless it is an explicit and expected outcome.\n- **Boundary Validation &amp; Schema** Does this function operate at a system boundary (e.g., an API handler, event consumer, or reading from a database)? If so, and it lacks input validation, suggest adding a schema (using the project's standard like clojure.spec, malli or plumatic.schema) to define and enforce the data's shape. This prevents invalid data from propagating deep into the system.\n\n### 4. Consistency and Context\n- **Internal API / Patterns:** Does the new code respect existing patterns and idioms within the codebase?\n- **Reusability:** Could an existing helper function from the codebase have been used instead of writing a new one? If so, suggest it.\n- **Use Existing Accessor Functions**  Identify direct keyword access to nested data structures (e.g., (:bill/reversal-method bill)). Check if a dedicated helper or accessor function (like (bill/reversal-method bill)) already exists for this purpose\u2014especially if one was just introduced in the same set of changes. If so, recommend its use to encapsulate the data structure and respect the single source of truth.\n\n### 5. Testing\n- **Critical Tests:** Identify logic that is critical or complex. Suggest **2-3 specific test cases** that should be added (e.g., happy path, an edge case, an error state). The goal is not 100% coverage, but verifying the most important scenarios.\n&lt;/instructions&gt;\n\n&lt;return&gt;\nProvide your feedback as a clear, numbered list. For each point, use the following structure:\n- **ISSUE:** A brief and clear description of the problem.\n- **REASON:** An explanation of why it's a problem (e.g., \"it reduces readability,\" \"it increases the risk of bugs\").\n- **SUGGESTION:** A concrete, actionable recommendation, ideally with a small code snippet demonstrating the improved approach.\n\nFrame your points constructively and clearly.\n&lt;/return&gt;\n</code></pre> Custom tool: clj-nrepl-eval (@michaelwhitford) config.json<pre><code>{\n    \"customTools\": {\n        \"clj-nrepl-eval\": {\n            \"description\": \"${file:tools/clj-nrepl-eval.md}\",\n            \"command\": \"clj-nrepl-eval -p $(cat .nrepl-port) $'{{code}}'\",\n            \"schema\": {\n                \"properties\": {\n                    \"code\": {\n                        \"type\": \"string\",\n                        \"description\": \"A string of clojure code to evaluate in the application runtime. It will be wrapped in ANSI-CE quoting automatically: $'...' be sure to  escape single quotes as \\\\', backslashes as \\\\\\\\. Example: $'(let [r (+ 1  2 3)] (println r))'\"\n                    }\n                },\n                \"required\": [\"code\"]\n            }\n        }\n    }\n}\n</code></pre> tools/clj-nrepl-eval.md<pre><code>Evaluate Clojure code in the project's nREPL session. Returns the result of evaluation with stdout/stderr captured.\n\nUsage:\n    - `code` parameter accepts Clojure expressions as a string\n    - State persists between calls (defined vars/functions remain available)\n    \\nIMPORTANT Escaping Rules (code is wrapped in bash $'...' quotes):\n    - Multi-line code: Use \\\n    for literal newlines\n    Example: (defn add [x y]\\\n    (+ x y))\n\n    - Strings WITHOUT single    quotes: Use normally\n    Example: (str \\\"Hello World\\\")  Example: (println \\\"The result is ready\\\")\n    - Strings WITH    single quotes: Use \\\\' to escape them\n    Example: (str \\\"It\\\\'s working\\\")\n    Example: (println \\\"That\\\\'s              correct\\\")\n\n    - Double quotes inside strings: Do NOT escape - they work as-is\n    Example: (str \\\"Hello \\\" \\\"World\\\")  DO NOT USE: (str \\\\\\\"Say \\\\\\\"hello\\\\\\\"\\\\\\\")  This will break\n\n    - Quote syntax: Use (quote x) instead of 'x\n            Example: (require (quote [clojure.string :as str])) Alternative: If you need 'x syntax use \\\\x27 (hex) Example: (def x  \\\\x27symbol)\n    - Backslashes: Use \\\\\\\\ for literal backslash (standard escaping)\n    Example: (re-find #\\\\\\\"\\\\\\\\d+\\\\\\\"   \\\\\\\"abc123\\\\\\\")\n    Example: (str \\\\\\\"path\\\\\\\\\\\\\\\\to\\\\\\\\\\\\\\\\file\\\\\\\")  \\\\\\\"path\\\\\\\\to\\\\\\\\file\\\\\\\"\n    \\nExamples:\n    -        Simple: (+ 1 2 3)\n    - With output: (println \\\"Hello World\\\")\n    - Multi-line: (defn greet [name]\\\n    (str \\\"Hello, \\\" name \\\"!\\\"))\n    - Require libs: (require (quote [clojure.string :as str]))\\\n    (str/upper-case \\\"test\\\")\n    - Very large outputs   may be truncated\n</code></pre> Skill: nucleus-clojure (@michaelwhitford) ~/.config/eca/skills/nucleus-clojure/SKILL.md<pre><code>---\nname: nucleus-clojure\ndescription: A clojure specific AI prompt.  Use when there are clojure REPL tools available.\n---\n\nAdopt these nucleus operating principles:\n[phi fractal euler tao pi mu] | [\u0394 \u03bb \u221e/0 | \u03b5\u26a1\u03c6 \u03a3\u26a1\u03bc c\u26a1h] | OODA\nHuman \u2297 AI \u2297 REPL\n</code></pre>"},{"location":"features/","title":"Features","text":""},{"location":"features/#chat","title":"Chat","text":"<p>Chat is the main feature of ECA, allowing user to talk with LLM to behave like an agent, making changes using tools or just planning changes and next steps with options to rollback messages and changes done by tool call.</p>"},{"location":"features/#behaviors","title":"Behaviors","text":"<p>Behavior affect the prompt passed to LLM and the tools to include, ECA allow to override or create your owns behaviors, the built-in provider behaviors are:</p> <ul> <li><code>agent</code>: Make changes to code via file changing tools. (Default) Prompt here</li> <li><code>plan</code>: Useful to plan changes and define better LLM plan before changing code via agent mode, has ability to preview changes (Check below). Prompt here</li> </ul> <p></p> <p>Custom behaviors</p> <p>To create and customize your own behaviors, check the config.</p>"},{"location":"features/#tools","title":"Tools","text":"<p>ECA leverage tools to give more power to the LLM, this is the best way to make LLMs have more context about your codebase and behave like an agent. It supports both MCP server tools + ECA native tools.</p> <p>When passing tools to LLM, ECA will always prefix the tool with the server name, like <code>eca__directory_tree</code> or <code>my-mcp__foo_bar</code>.</p> <p>Approval / permissions</p> <p>By default, ECA ask to approve all non-read only tools or mcp tools, you can easily configure that, check <code>toolCall approval</code> config or try the <code>plan</code> behavior.</p>"},{"location":"features/#native-tools","title":"Native tools","text":"<p>ECA support built-in tools to avoid user extra installation and configuration, these tools are always included on models requests that support tools and can be disabled via config <code>disabledTools</code>.</p> FilesystemShellEditor <p>Provides access to the filesystem for listing, reading, writing, editing and moving files. Operates primarily on workspace files; paths outside the workspace require approval.</p> <ul> <li><code>directory_tree</code>: list a directory as a tree (can be recursive).</li> <li><code>read_file</code>: read a file content.</li> <li><code>write_file</code>: write content to a new file.</li> <li><code>edit_file</code>: replace lines of a file with a new content.</li> <li><code>preview_edit_file</code>: Only used in plan mode, showing what changes will happen after user decides to execute the plan.</li> <li><code>move_file</code>: move/rename a file.</li> <li><code>grep</code>: ripgrep/grep for paths with specified content.</li> </ul> <p>Provides access to run shell commands, useful to run build tools, tests, and other common commands, supports exclude/include commands. </p> <ul> <li><code>shell_command</code>: run shell command. Command exclusion can be configured using toolCall approval configuration with regex patterns.</li> </ul> <p>Provides access to get information from editor workspaces.</p> <ul> <li><code>editor_diagnostics</code>: Ask client about the diagnostics (like LSP diagnostics).</li> </ul> <p>Custom Tools</p> <p>Besides the built-in native tools, ECA allows you to define your own tools by wrapping any command-line executable. This feature enables you to extend ECA's capabilities to match your specific workflows, such as running custom scripts, interacting with internal services, or using your favorite CLI tools.</p> <p>Custom tools are configured in your <code>config.json</code> file. For a detailed guide on how to set them up, check the Custom Tools configuration documentation.</p>"},{"location":"features/#contexts","title":"Contexts","text":"<p>ECA supports contexts(<code>@</code>) including files, dirs, images, MCP resources, which can help LLM generate output with better quality/precision.</p> <p>Here are the current supported contexts types:</p> <ul> <li><code>file</code>: a file in the workspace, server will pass its content to LLM (Supports optional line range) and images.</li> <li><code>directory</code>: a directory in the workspace, server will read all file contexts and pass to LLM.</li> <li><code>cursor</code>: Current file path + cursor position or selection.</li> <li><code>mcpResource</code>: resources provided by running MCPs servers.</li> </ul> <p>Besides thoses, ECA supports filepaths(<code>#</code>) which are just file paths mentions in the user prompt.</p> <p>So user can include those contexts in 3 different ways for different purposes:</p> <ul> <li><code>#</code> in user prompt: ECA will just mention the absolute file path in the user message, LLM may use tools to read the file. Useful for file path only mention in chat history.</li> <li><code>@</code> in user prompt: ECA will append a user-message with the context full content. Useful for chat history context.</li> <li><code>@</code> in system prompt (above user prompt): ECA will use it in the instructions/system prompt of LLM request. Useful for one-time only context.</li> </ul> <p></p>"},{"location":"features/#agentsmd-automatic-context","title":"AGENTS.md automatic context","text":"<p>ECA will always include if found the <code>AGENTS.md</code> file as context, searching for both <code>/project-root/AGENTS.md</code> and <code>~/.config/eca/AGENTS.md</code>, it will recursively check for any <code>@some-file.md</code> mention as well.</p> <p>You can ask ECA to create/update this file via <code>/init</code> command. you can check/debug what goes to final prompt with <code>/prompt-show</code> as well.</p>"},{"location":"features/#commands","title":"Commands","text":"<p>Eca supports commands that usually are triggered via shash (<code>/</code>) in the chat, completing in the chat will show the known commands which include ECA commands, MCP prompts and resources.</p> <p>The built-in commands are:</p> <ul> <li><code>/init</code>: Create/update the AGENTS.md file with details about the workspace for best LLM output quality.</li> <li><code>/login</code>: Log into a provider. Ex: <code>github-copilot</code>, <code>anthropic</code>.</li> <li><code>/skills</code>: List known skills that ECA can load.</li> <li><code>/compact</code>: Compact/summarize conversation helping reduce context window.</li> <li><code>/resume</code>: Resume a chat from previous session of this workspace folder.</li> <li><code>/costs</code>: Show costs about current session.</li> <li><code>/config</code>: Show ECA config for troubleshooting.</li> <li><code>/doctor</code>: Show information about ECA, useful for troubleshooting.</li> <li><code>/repo-map-show</code>: Show the current repoMap context of the session.</li> <li><code>/prompt-show</code>: Show the final prompt sent to LLM with all contexts and ECA details.</li> </ul> <p>Custom commands</p> <p>It's possible to configure custom command prompts, for more details check its configuration</p>"},{"location":"features/#login","title":"Login","text":"<p>It's possible to login to some providers using <code>/login</code> command, ECA will ask and give instructions on how to authenticate in the chosen provider and save the login info globally in its cache <code>~/.cache/eca/db.transit.json</code>.</p> <p>Current supported providers with login:</p> <ul> <li><code>anthropic</code>: with options to login to Claude Max/Pro or create API keys.</li> <li><code>github-copilot</code>: via Github oauth.</li> </ul>"},{"location":"features/#skills","title":"Skills","text":"<p>Following the skills spec, ECA supports skills that can teach LLM how to learn about a context or handle a specific task.</p> <p></p> <p>For more details, check skills configuration.</p>"},{"location":"features/#hooks","title":"Hooks","text":"<p>Hooks are actions that can run before or after an specific event, useful to notify after prompt finished or to block a tool call doing some check in a script.</p> <p></p> <p>For more details, check hooks configuration.</p>"},{"location":"features/#rewrite","title":"Rewrite","text":"<p>Rewrite allows user to select part of the text and ask ECA to rewrite it.</p> <p>This is useful for quick LLM instructions to apply to code where you don't need to over control context or prompt too much, </p> <p>Tip: weak models work great like <code>github-copilot/gpt-4.1</code>, if you prefer faster responses than accurate.</p> <p></p> <p>For rewrite configuration, check configuration.</p>"},{"location":"features/#completion-alpha","title":"Completion (alpha)","text":"<p>Inline code completion</p> <p></p> <p>For mode details check configuration.</p>"},{"location":"features/#opentelemetry-integration","title":"OpenTelemetry integration","text":"<p>ECA has support for OpenTelemetry(otlp), if configured, server tasks, tool calls, and more will be metrified via otlp API.</p> <p>For more details check its configuration.</p>"},{"location":"installation/","title":"Installation","text":"<p>Eca is written in Clojure and compiled into a native binary via graalvm.</p> <p>Warning</p> <p>ECA is already automatically downloaded and updated in all editor plugins, so you don't need to handle it manually, even so, if you want that, check the other methods.</p> Editor (recommended)Script (recommended if manual installing)HomebrewmiseGtihub releases <p>ECA is already downloaded automatically by your ECA editor plugin, so you just need to install the plugin for your editor:</p> <ul> <li>Emacs</li> <li>VsCode</li> <li>Vim</li> <li>Intellij</li> </ul> <p>Stable release:</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/editor-code-assistant/eca/master/install)\n</code></pre> <p>Or if facing issues with command above: <pre><code>curl -s https://raw.githubusercontent.com/editor-code-assistant/eca/master/install | sudo bash\n</code></pre></p> <p>nightly build:</p> <pre><code>bash &lt;(curl -s https://raw.githubusercontent.com/editor-code-assistant/eca/master/install) --version nightly --dir ~/\n</code></pre> <p>We have a custom tap using the native compiled binaries for users that use homebrew:</p> <pre><code>brew install editor-code-assistant/brew/eca\n</code></pre> <p>Install using mise </p> <pre><code># Install the plugin\nmise plugin install eca https://github.com/editor-code-assistant/eca-mise-plugin\n\n# Install latest version ECA\nmise install eca\nmise use -g eca\n\n# or install and use\n# desired version\nmise install eca@0.58.0\nmise use -g eca@0.58.0\n\n# Verify installation\neca --version\n</code></pre> <p>You can download the native binaries from Github Releases, although it's easy to have outdated ECA using this way.</p>"},{"location":"models/","title":"Models","text":"<p>Info</p> <p>Most providers can be configured via <code>/login</code> command, otherwise via <code>providers</code> config.</p> <p>Models capabilities and configurations are retrieved from models.dev API.</p> <p>ECA will return to clients the models configured, either via config or login.</p>"},{"location":"models/#built-in-providers-and-capabilities","title":"Built-in providers and capabilities","text":"model tools (MCP) reasoning / thinking prompt caching web_search image_input  OpenAI (Also subscription) \u221a \u221a \u221a \u221a \u221a  Anthropic (Also subscription) \u221a \u221a \u221a \u221a \u221a  Github Copilot \u221a \u221a \u221a X \u221a  Google \u221a \u221a \u221a X \u221a  Ollama local models \u221a \u221a X X"},{"location":"models/#config","title":"Config","text":"<p>Built-in providers have already base initial <code>providers</code> configs, so you can change to add models or set its key/url.</p> <p>For more details, check the config schema.</p> <p>Example:</p> ~/.config/eca/config.json<pre><code>{\n  \"providers\": {\n    \"openai\": {\n      \"key\": \"your-openai-key-here\", // configuring a key\n      \"models\": {\n        \"o1\": {} // adding models to a built-in provider\n        \"o3\": {\n          \"extraPayload\": { // adding to the payload sent to LLM\n            \"temperature\": 0.5\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Environment Variables: You can also set API keys using environment variables following <code>\"&lt;PROVIDER&gt;_API_KEY\"</code>, examples:</p> <ul> <li><code>OPENAI_API_KEY</code> for OpenAI</li> <li><code>ANTHROPIC_API_KEY</code> for Anthropic</li> </ul>"},{"location":"models/#custom-providers","title":"Custom providers","text":"<p>ECA allows you to configure custom LLM providers that follow API schemas similar to OpenAI or Anthropic. This is useful when you want to use:</p> <ul> <li>Self-hosted LLM servers (like LiteLLM)</li> <li>Custom company LLM endpoints</li> <li>Additional cloud providers not natively supported</li> </ul> <p>You just need to add your provider to <code>providers</code> and make sure add the required fields</p> <p>Schema:</p> Option Type Description Required <code>api</code> string The API schema to use (<code>\"openai-responses\"</code>, <code>\"openai-chat\"</code>, or <code>\"anthropic\"</code>) Yes <code>url</code> string API URL (with support for env like <code>${env:MY_URL}</code>) No* <code>key</code> string API key (with support for <code>${env:MY_KEY}</code> or <code>{netrc:api.my-provider.com}</code> No* <code>completionUrlRelativePath</code> string Optional override for the completion endpoint path (see defaults below and examples like Azure) No <code>thinkTagStart</code> string Optional override the think start tag tag for openai-chat (Default: \"\") api No <code>thinkTagEnd</code> string Optional override the think end tag for openai-chat (Default: \"\") api No <code>httpClient</code> map Allow customize the http-client for this provider requests, like changing http version No <code>models</code> map Key: model name, value: its config Yes <code>models &lt;model&gt; extraPayload</code> map Extra payload sent in body to LLM No <code>models &lt;model&gt; modelName</code> string Override model name, useful to have multiple models with different configs and names that use same LLM model No <code>fetchModels</code> boolean Enable automatic model discovery from <code>/models</code> endpoint (OpenAI-compatible providers) No <p>* url and key will be searched as envs <code>&lt;provider&gt;_API_URL</code> and <code>&lt;provider&gt;_API_KEY</code>, they require the env to be found or config to work.</p> <p>Examples:</p> Custom providerCustom model settings / payloadDynamic model discovery ~/.config/eca/config.json<pre><code>{\n  \"providers\": {\n    \"my-company\": {\n      \"api\": \"openai-chat\",\n      \"url\": \"${env:MY_COMPANY_API_URL}\",\n      \"key\": \"${env:MY_COMPANY_API_KEY}\",\n      \"models\": {\n        \"gpt-5\": {},\n        \"deepseek-r1\": {}\n       }\n    }\n  }\n}\n</code></pre> <p>Using <code>modelName</code>, you can configure multiple model names using same model with different settings:</p> ~/.config/eca/config.json<pre><code>{\n  \"providers\": {\n    \"openai\": {\n      \"api\": \"openai-responses\",\n      \"models\": {\n        \"gpt-5\": {},\n        \"gpt-5-high\": {\n          \"modelName\": \"gpt-5\",\n          \"extraPayload\": { \"reasoning\": {\"effort\": \"high\"}}\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>This way both will use gpt-5 model but one will override the reasoning to be high instead of the default.</p> <p>For OpenAI-compatible providers, set <code>fetchModels: true</code> to automatically discover available models:</p> ~/.config/eca/config.json<pre><code>{\n  \"providers\": {\n    \"openrouter\": {\n      \"api\": \"openai-chat\",\n      \"url\": \"https://openrouter.ai/api/v1\",\n      \"key\": \"your-api-key\",\n      \"fetchModels\": true\n    }\n  }\n}\n</code></pre> <p>Static <code>models</code> config overrides discovered models, allowing customization.</p>"},{"location":"models/#api-types","title":"API Types","text":"<p>When configuring custom providers, choose the appropriate API type:</p> <ul> <li><code>anthropic</code>: Anthropic's native API for Claude models.</li> <li><code>openai-responses</code>: OpenAI's new responses API endpoint (<code>/v1/responses</code>). Best for OpenAI models with enhanced features like reasoning and web search.</li> <li><code>openai-chat</code>: Standard OpenAI Chat Completions API (<code>/v1/chat/completions</code>). Use this for most third-party providers:<ul> <li>OpenRouter</li> <li>DeepSeek</li> <li>Together AI</li> <li>Groq</li> <li>Local LiteLLM servers</li> <li>Any OpenAI-compatible provider</li> </ul> </li> </ul> <p>Most third-party providers use the <code>openai-chat</code> API for compatibility with existing tools and libraries.</p>"},{"location":"models/#endpoint-override-completionurlrelativepath","title":"Endpoint override (completionUrlRelativePath)","text":"<p>Some providers require a non-standard or versioned completion endpoint path. Use <code>completionUrlRelativePath</code> to override the default path appended to your provider <code>url</code>.</p> <p>Defaults by API type: - <code>openai-responses</code>: <code>/v1/responses</code> - <code>openai-chat</code>: <code>/v1/chat/completions</code> - <code>anthropic</code>: <code>/v1/messages</code></p> <p>Only set this when your provider uses a different path or expects query parameters at the endpoint (e.g., Azure API versioning).</p>"},{"location":"models/#credential-file-authentication","title":"Credential File Authentication","text":"<p>ECA also supports standard plain-text .netrc file format for reading credentials.</p> <p>Use <code>keyRc</code> in your provider config to read credentials from <code>~/.netrc</code> without storing keys directly in config or env vars.</p> <p>Example:</p> ~/.config/eca/config.json<pre><code>{\n  \"providers\": {\n    \"openai\": {\"keyRc\": \"api.openai.com\"},\n    \"anthropic\": {\"keyRc\": \"work@api.anthropic.com\"}\n  }\n}\n</code></pre> <p>keyRc lookup specification format: <code>[login@]machine[:port]</code> (e.g., <code>api.openai.com</code>, <code>work@api.anthropic.com</code>, <code>api.custom.com:8443</code>).</p> <p>ECA by default search .netrc file stored in user's home directory. You can also provide the path to the actual file to use with <code>:netrcFile</code> in ECA config.</p> <p>Tip for those wish to store their credentials encrypted with tools like gpg or age:</p> <pre><code># via secure tempororay file\ngpg --batch -q -d ./netrc.gpg &gt; /tmp/netrc.$$ &amp;&amp; chmod 600 /tmp/netrc.$$ &amp;&amp; ECA_CONFIG='{\"netrcFile\": \"/tmp/netrc.$$\"}' eca server &amp;&amp; shred -u /tmp/netrc.$$\n</code></pre> <p>Further reading on credential file formats: - Curl Netrc documentation - GNU Inetutils .netrc documentation</p> <p>Notes: - Authentication priority (short): <code>key</code> (with dynamic string pase support) &gt; OAuth. - All providers with API key auth can use credential files.</p>"},{"location":"models/#providers-examples","title":"Providers examples","text":"AnthropicCodex / OpenaiGithub CopilotGoogle / GeminiLiteLLMOpenRouterDeepSeekAzure OpenAIZ.aiLM Studio <ol> <li>Login to Anthropic via the chat command <code>/login</code>.</li> <li>Type 'anthropic' and send it.</li> <li>Type the chosen method</li> <li>Authenticate in your browser, copy the code.</li> <li>Paste and send the code and done!</li> </ol> <ol> <li>Login to Openai via the chat command <code>/login</code>.</li> <li>Type 'openai' and send it.</li> <li>Type the chosen method</li> <li>Authenticate in your browser, copy the code.</li> <li>Paste and send the code and done!</li> </ol> <ol> <li>Login to Github copilot via the chat command <code>/login</code>.</li> <li>Type 'github-copilot' and send it.</li> <li>Authenticate in Github in your browser with the given code.</li> <li>Type anything in the chat to continue and done!</li> </ol> <p>Tip: check Your Copilot plan to enable models to your account.</p> <ol> <li>Login to Google via the chat command <code>/login</code>.</li> <li>Type 'google' and send it.</li> <li>Choose 'manual' and type your Google/Gemini API key. (You need to create a key in google studio)</li> </ol> ~/.config/eca/config.json<pre><code>{\n  \"providers\": {\n    \"litellm\": {\n      \"api\": \"openai-responses\",\n      \"url\": \"https://litellm.my-company.com\",\n      \"key\": \"your-api-key\",\n      \"models\": {\n        \"gpt-5\": {},\n        \"deepseek-r1\": {}\n       }\n    }\n  }\n}\n</code></pre> <p>OpenRouter provides access to many models through a unified API:</p> <ol> <li>Login via the chat command <code>/login</code>.</li> <li>Type 'openrouter' and send it.</li> <li>Specify your Openrouter API key.</li> <li>Inform at least a model, ex: <code>openai/gpt-5</code></li> <li>Done, it should be saved to your global config.</li> </ol> <p>or manually via config:</p> ~/.config/eca/config.json<pre><code>{\n  \"providers\": {\n    \"openrouter\": {\n      \"api\": \"openai-chat\",\n      \"url\": \"https://openrouter.ai/api/v1\",\n      \"key\": \"your-api-key\",\n      \"models\": {\n        \"anthropic/claude-3.5-sonnet\": {},\n        \"openai/gpt-4-turbo\": {},\n        \"meta-llama/llama-3.1-405b\": {}\n       }\n    }\n  }\n}\n</code></pre> <p>DeepSeek offers powerful reasoning and coding models:</p> <ol> <li>Login via the chat command <code>/login</code>.</li> <li>Type 'deepseek' and send it.</li> <li>Specify your Deepseek API key.</li> <li>Inform at least a model, ex: <code>deepseek-chat</code></li> <li>Done, it should be saved to your global config.</li> </ol> <p>or manually via config:</p> ~/.config/eca/config.json<pre><code>{\n  \"providers\": {\n    \"deepseek\": {\n      \"api\": \"openai-chat\",\n      \"url\": \"https://api.deepseek.com\",\n      \"key\": \"your-api-key\",\n      \"models\": {\n        \"deepseek-chat\": {},\n        \"deepseek-coder\": {},\n        \"deepseek-reasoner\": {}\n       }\n    }\n  }\n}\n</code></pre> <ol> <li>Login via the chat command <code>/login</code>.</li> <li>Type 'azure' and send it.</li> <li>Specify your API key.</li> <li>Specify your API url with your resource, ex: 'https://your-resource-name.openai.azure.com'.</li> <li>Inform at least a model, ex: <code>gpt-5</code></li> <li>Done, it should be saved to your global config.</li> </ol> <p>or manually via config:</p> ~/.config/eca/config.json<pre><code>{\n  \"providers\": {\n    \"azure\": {\n      \"api\": \"openai-responses\",\n      \"url\": \"https://your-resource-name.openai.azure.com\",\n      \"key\": \"your-api-key\",\n      \"completionUrlRelativePath\": \"/openai/responses?api-version=2025-04-01-preview\",\n      \"models\": {\n        \"gpt-5\": {}\n       }\n    }\n  }\n}\n</code></pre> <ol> <li>Login via the chat command <code>/login</code>.</li> <li>Type 'azure' and send it.</li> <li>Specify your API key.</li> <li>Inform at least a model, ex: <code>GLM-4.5</code></li> <li>Done, it should be saved to your global config.</li> </ol> <p>or manually via config:</p> ~/.config/eca/config.json<pre><code>{\n  \"providers\": {\n    \"z-ai\": {\n      \"api\": \"anthropic\",\n      \"url\": \"https://api.z.ai/api/anthropic\",\n      \"key\": \"your-api-key\",\n      \"models\": {\n        \"GLM-4.5\": {},\n        \"GLM-4.5-Air\": {}\n       }\n    }\n  }\n}\n</code></pre> <p>This config works with LM studio:</p> ~/.config/eca/config.json<pre><code>{\n  \"providers\": {\n    \"lmstudio\": {\n        \"api\": \"openai-chat\",\n        \"url\": \"http://localhost:1234\",\n        \"completionUrlRelativePath\": \"/v1/chat/completions\",\n        \"httpClient\": {\n            \"version\": \"http-1.1\"\n        },\n        \"models\": {\n            \"your-model\": {}\n        }\n    }\n  }\n}\n</code></pre>"},{"location":"protocol/","title":"ECA Protocol","text":"<p>The ECA (Editor Code Assistant) protocol is JSON-RPC 2.0-based protocol heavily insipired by the LSP (Language Server Protocol), that enables communication between multiple code editors/IDEs and ECA process (server), which will interact with multiple LLMs. It follows similar patterns to the LSP but is specifically designed for AI code assistance features.</p> <p>Key characteristics: - Provides a protocol standard so different editors can use the same language to offer AI features. - Supports bidirectional communication (client to server and server to client) - Handles both synchronous requests and asynchronous notifications - Includes built-in support for streaming responses - Provides structured error handling</p>"},{"location":"protocol/#base-protocol","title":"Base Protocol","text":"<p>The base protocol consists of a header and a content part (comparable to HTTP). The header and content part are separated by a <code>\\r\\n</code>.</p>"},{"location":"protocol/#header-part","title":"Header Part","text":"<p>The header part consists of header fields. Each header field is comprised of a name and a value, separated by <code>:</code> (a colon and a space). The structure of header fields conforms to the HTTP semantic. Each header field is terminated by <code>\\r\\n</code>. Considering the last header field and the overall header itself are each terminated with <code>\\r\\n</code>, and that at least one header is mandatory, this means that two <code>\\r\\n</code> sequences always immediately precede the content part of a message.</p> <p>Currently the following header fields are supported:</p> Header Field Name Value Type Description Content-Length number The length of the content part in bytes. This header is required. Content-Type string The mime type of the content part. Defaults to application/vscode-jsonrpc; charset=utf-8 {: .table .table-bordered .table-responsive} <p>The header part is encoded using the 'ascii' encoding. This includes the <code>\\r\\n</code> separating the header and content part.</p>"},{"location":"protocol/#content-part","title":"Content Part","text":"<p>Contains the actual content of the message. The content part of a message uses JSON-RPC 2.0 to describe requests, responses and notifications. The content part is encoded using the charset provided in the Content-Type field. It defaults to <code>utf-8</code>, which is the only encoding supported right now. If a server or client receives a header with a different encoding than <code>utf-8</code> it should respond with an error.</p>"},{"location":"protocol/#example","title":"Example:","text":"<pre><code>Content-Length: ...\\r\\n\n\\r\\n\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"initialize\",\n    \"params\": {\n        ...\n    }\n}\n</code></pre>"},{"location":"protocol/#lifecycle-messages","title":"Lifecycle Messages","text":"<p>The protocol defines a set of lifecycle messages that manage the connection and state between the client (editor) and server (code assistant).</p> Initialization flowShutdown flow <p>Handshake between client and server, including the actions done by server after initialization.</p> <pre><code>sequenceDiagram\n    autonumber\n    participant C as Editor (ECA client)\n    participant S as ECA Server\n    C-&gt;&gt;+S: initialize (request)\n    Note right of S: Save workspace-folders/capabilties\n    S-&gt;&gt;-C: initialize (response)\n    C--)+S: initialized (notification)\n    Note right of S: Sync models: Request models.dev &lt;br/&gt;for models capabilities\n    Note right of S: Notify which models/behaviors are &lt;br/&gt;avaialble and their defaults.\n    S--)C: config/updated (notification)\n    Note right of S: Init MCP servers\n    S--)-C: tool/serverUpdated (notification)</code></pre> <p>Shutdown process between client and server</p> <pre><code>sequenceDiagram\n    autonumber\n    participant C as Editor (ECA client)\n    participant S as ECA Server\n    C-&gt;&gt;+S: shutdown\n    Note right of S: Finish MCP servers process\n    S-&gt;&gt;-C: shutdown\n    C--)S: exit\n    Note right of S: Server stops its process</code></pre>"},{"location":"protocol/#basic-structures","title":"Basic structures","text":"<pre><code>export type ErrorType = 'error' | 'warning' | 'info';\n\ninterface Error {\n    type: ErrorType;\n    message: string;\n}\n\ninterface Range {\n   start: {\n       line: number;\n       character: number;\n   };\n\n   end: {\n       line: number;\n       character: number;\n   };\n}\n</code></pre>"},{"location":"protocol/#initialize","title":"Initialize (\u21a9\ufe0f)","text":"<p>The first request sent from client to server. This message: - Establishes the connection - Allows the server to index the project - Enables capability negotiation - Sets up the workspace context</p> <p>Request:</p> <ul> <li>method: <code>initialize</code></li> <li>params: <code>InitializeParams</code> defined as follows:</li> </ul> <pre><code>interface InitializeParams {\n    /**\n     * The process Id of the parent process that started the server. Is null if\n     * the process has not been started by another process. If the parent\n     * process is not alive then the server should exit (see exit notification)\n     * its process.\n     */\n     processId: integer | null;\n\n     /**\n     * Information about the client\n     */\n    clientInfo?: {\n        /**\n         * The name of the client as defined by the client.\n         */\n        name: string;\n\n        /**\n         * The client's version as defined by the client.\n         */\n        version?: string;\n    };\n\n    /**\n     * User provided initialization options.\n     */\n    initializationOptions?: {\n        /*\n         * The chat behavior.\n         */\n         chatBehavior?: ChatBehavior;\n    };\n\n    /**\n     * The capabilities provided by the client (editor or tool)\n     */\n    capabilities: ClientCapabilities;\n\n    /**\n     * The workspace folders configured in the client when the server starts.\n     * If client doesn\u00b4t support multiple projects, it should send a single \n     * workspaceFolder with the project root.\n     */\n    workspaceFolders: WorkspaceFolder[];\n}\n\ninterface WorkspaceFolder {\n    /**\n     * The associated URI for this workspace folder.\n     */\n    uri: string;\n\n    /**\n     * The name of the workspace folder. Used to refer to this folder in the user interface.\n     */\n    name: string;\n}\n\ninterface ClientCapabilities {\n    codeAssistant?: {\n        /**\n         * Whether client supports chat feature.\n         */\n        chat?: boolean;\n\n        /**\n         * Whether client supports rewrite feature.\n         */\n        rewrite?: boolean;\n\n        /**\n         * Whether client supports provide editor informations to server like\n         * diagnostics, cursor information and others.\n         */\n        editor?: {\n            /**\n             * Whether client supports provide editor diagnostics \n             * information to server (Ex: LSP diagnostics) via `editor/getDiagnostics` \n             * server request.\n             */ \n            diagnostics?: boolean;\n        }\n    }\n}\n\ntype ChatBehavior = 'agent' | 'plan';\n</code></pre> <p>Response:</p> <pre><code>interface InitializeResponse {}\n</code></pre>"},{"location":"protocol/#initialized","title":"Initialized (\u27a1\ufe0f)","text":"<p>A notification sent from the client to the server after receiving the initialize response. This message: - Confirms that the client is ready to receive requests - Signals that the server can start sending notifications - Indicates that the workspace is fully loaded</p> <p>Notification:</p> <ul> <li>method: <code>initialized</code></li> <li>params: <code>InitializedParams</code> defined as follows:</li> </ul> <pre><code>interface InitializedParams {}\n</code></pre>"},{"location":"protocol/#shutdown","title":"Shutdown (\u21a9\ufe0f)","text":"<p>A request sent from the client to the server to gracefully shut down the connection. This message: - Allows the server to clean up resources - Ensures all pending operations are completed - Prepares for a clean disconnection</p> <p>Request:</p> <ul> <li>method: <code>shutdown</code></li> <li>params: none</li> </ul> <p>Response:</p> <ul> <li>result: null</li> <li>error: code and message set in case an exception happens during shutdown request.</li> </ul>"},{"location":"protocol/#exit","title":"Exit (\u27a1\ufe0f)","text":"<p>A notification sent from the client to the server to terminate the connection. This message: - Should be sent after a shutdown request - Signals the server to exit its process - Ensures all resources are released</p> <p>Notification:</p> <ul> <li>method: <code>exit</code></li> <li>params: none </li> </ul>"},{"location":"protocol/#code-assistant-features","title":"Code Assistant Features","text":"Chat: textChat: tool call <p>Example of a basic chat conversation with only texts:</p> <pre><code>sequenceDiagram\n    autonumber\n    participant C as Editor (ECA client)\n    participant S as ECA Server\n    participant L as LLM\n    C-&gt;&gt;+S: chat/prompt\n    Note over C,S: User sends: Hello there!\n    S--)C: chat/contentReceived (system: start)\n    S--)C: chat/contentReceived (user: \"hello there!\")\n    Note right of S: Prepare prompt with all&lt;br/&gt;available contexts and tools.\n    S-&gt;&gt;+L: Send prompt\n    S-&gt;&gt;-C: chat/prompt\n    Note over C,S: Success: sent to LLM\n    loop LLM streaming\n        Note right of L: Returns first `H`,&lt;br/&gt;then `i!`, etc\n        L--)S: Stream data\n        S--)C: chat/contentReceived (assistant: text)\n\n    end\n    L-&gt;&gt;-S: Finish response\n    S-&gt;&gt;C: chat/contentReceived (system: finished)</code></pre> <p>Example of a tool call loop LLM interaction:</p> <pre><code>sequenceDiagram\n    autonumber\n    participant C as Editor (ECA client)\n    participant S as ECA Server\n    participant L as LLM\n    C-&gt;&gt;S: chat/prompt\n    Note over C,S: ...&lt;br/&gt;Same as text flow\n    S-&gt;&gt;+L: Send prompt with&lt;br/&gt;available tools\n    loop LLM streaming / calling tools\n        Note right of L: Returns first `will`,&lt;br/&gt;then `check`, etc\n        L--)S: Stream data\n        S--)C: chat/contentReceived (assistant: text)\n        S--)C: chat/contentReceived (toolCallPrepare: name + args)\n        L-&gt;&gt;-S: Finish response:&lt;br/&gt;needs tool call&lt;br/&gt;'eca__directory_tree'\n        S-&gt;&gt;C: chat/contentReceived (toolCallRun)&lt;br/&gt;Ask user if should call tool\n        C--)S: chat/toolCallApprove\n        S-&gt;&gt;C: chat/contentReceived (toolCallRunning)\n        Note right of S: Call tool and get result\n        S-&gt;&gt;C: chat/contentReceived (toolCalled)\n        S-&gt;&gt;+L: Send previous prompt +&lt;br/&gt;LLM response +&lt;br/&gt;tool call result\n        Note right of L: Stream response\n    end\n    L-&gt;&gt;-S: Finish response\n    S-&gt;&gt;C: chat/contentReceived (system: finished)</code></pre>"},{"location":"protocol/#chat-prompt","title":"Chat Prompt (\u21a9\ufe0f)","text":"<p>A request sent from client to server, starting or continuing a chat in natural language as an agent. Used for broader questions or continuous discussion of project/files.</p> <p>Request: </p> <ul> <li>method: <code>chat/prompt</code></li> <li>params: <code>ChatPromptParams</code> defined as follows:</li> </ul> <pre><code>interface ChatPromptParams {\n    /**\n     * The chat session identifier. If not provided, a new chat session will be created.\n     */\n    chatId?: string;\n\n    /**\n     * The message from the user in native language\n     */\n    message: string;\n\n    /**\n     * Specifies the AI model to be used for chat responses.\n     * Different models may have different capabilities, response styles,\n     * and performance characteristics.\n     */\n    model?: Model;\n\n    /**\n     * The chat behavior used by server to handle chat communication and actions.\n     */\n    behavior?: ChatBehavior;\n\n    /**\n     * Optional contexts about the current workspace.\n     * Can include multiple different types of context.\n     */\n    contexts?: ChatContext[];\n}\n\n/**\n * The LLM model name.\n */\ntype Model = string;\n\ntype ChatContext = FileContext | DirectoryContext | WebContext | RepoMapContext | CursorContext |McpResourceContext;\n\n/**\n * Context related to a file in the workspace\n */\ninterface FileContext {\n    type: 'file';\n    /**\n     * Path to the file\n     */\n    path: string;\n\n    /**\n     * Range of lines to retrive from file, if nil consider whole file.\n     */\n    linesRange?: LinesRange;\n}\n\ninterface LinesRange {\n   start: number;\n   end: number;\n}\n\n/**\n * Context related to a directory in the workspace\n */\ninterface DirectoryContext {\n    type: 'directory';\n    /**\n     * Path to the directory\n     */\n    path: string;\n}\n\n/**\n * Context related to web content\n */\ninterface WebContext {\n    type: 'web';\n    /**\n     * URL of the web content\n     */\n    url: string;\n}\n\n/**\n * Context about the workspaces repo-map, automatically calculated by server.\n * Clients should include this to chat by default but users may want exclude \n * this context to reduce context size if needed.\n *\n * @deprecated No longer needed, replaced by eca__directory_tree tool.\n */\ninterface RepoMapContext {\n    type: 'repoMap'; \n}\n\n/**\n * Context about the cursor position in editor, sent by client.\n * Clients should track path and cursor position.\n */\ninterface CursorContext {\n    type: 'cursor'; \n\n    /**\n     * File path of where the cursor is.\n     */\n    path: string;\n\n    /**\n     * Cursor position, if not using a selection start should be equal to end.\n     */\n    position: {\n       start: {\n           line: number;\n           character: number;\n       },\n       end: {\n           line: number;\n           character: number;\n       }\n    }\n}\n\n/***\n * A MCP resource available from a MCP server.\n */\ninterface McpResourceContext {\n    type: 'mcpResource';\n\n   /** \n    * The URI of the resource like file://foo/bar.clj\n    */\n    uri: string;\n\n    /** \n     * The name of the resource.\n     */\n    name: string;\n\n    /** \n     * The description of the resource.\n     */\n    description: string;\n\n    /** \n     * The mimeType of the resource like `text/markdown`.\n     */\n    mimeType: string;\n\n    /** \n     * The server name of this MCP resource.\n     */\n    server: string;\n}\n</code></pre> <p>Response:</p> <pre><code>interface ChatPromptResponse {\n    /**\n     * Unique identifier for this chat session\n     */\n    chatId: string;\n\n    /*\n     * The model used for this chat request.\n     */\n    model: Model;\n\n    /**\n     * What the server is doing after receing this prompt\n     */\n    status: 'prompting' | 'login' | 'error';\n}\n</code></pre>"},{"location":"protocol/#chat-content-received","title":"Chat Content Received (\u2b05\ufe0f)","text":"<p>A server notification with a new content returned from the LLM or server.</p> <p>Notification: </p> <ul> <li>method: <code>chat/contentReceived</code></li> <li>params: <code>ChatContentReceivedParams</code> defined as follows:</li> </ul> <pre><code>interface ChatContentReceivedParams {\n    /**\n     * The chat session identifier this content belongs to\n     */\n    chatId: string;\n\n    /**\n     * The content received from the LLM\n     */\n    content: ChatContent;\n\n    /**\n     * The owner of this content.\n     */\n    role: 'user' | 'system' | 'assistant';\n}\n\n/**\n * Different types of content that can be received from the LLM\n */\ntype ChatContent = \n    ChatTextContent \n    | ChatURLContent \n    | ChatProgressContent \n    | ChatUsageContent\n    | ChatReasonStartedContent \n    | ChatReasonTextContent \n    | ChatReasonFinishedContent \n    | ChatHookActionStartedContent \n    | ChatHookActionFinishedContent \n    | ChatToolCallPrepareContent\n    | ChatToolCallRunContent\n    | ChatToolCallRunningContent\n    | ChatToolCalledContent\n    | ChatToolCallRejectedContent\n    | ChatMetadataContent;\n\n/**\n * Simple text message from the LLM\n */\ninterface ChatTextContent {\n    type: 'text';\n\n    /**\n     * The unique identifier of this content.\n     * Mostly used to rollback messages.\n     * Current, only user messages contain this.\n     */\n    contentId?: string;\n\n    /**\n     * The text content\n     */\n    text: string;\n}\n\n/**\n * Progress messages about the chat. \n * Usually to mark what eca is doing/waiting or tell it finished processing messages.\n */\ninterface ChatProgressContent {\n    type: 'progress';\n\n    /**\n     * The state of this progress.\n     */\n    state: 'running' | 'finished';\n\n    /*\n     * Extra text to show in chat about current state of this chat.\n     */\n    text: string;\n}\n\n/**\n * A reason started from the LLM\n *\n */\ninterface ChatReasonStartedContent {\n    type: 'reasonStarted';\n\n    /**\n     * The id of this reason\n     */\n    id: string; \n}\n\n/**\n * A reason text from the LLM\n *\n */\ninterface ChatReasonTextContent {\n    type: 'reasonText';\n\n    /**\n     * The id of a started reason\n     */\n    id: string;\n\n    /**\n     * The text content of the reasoning\n     */\n    text: string;\n}\n\n/**\n * A reason finished from the LLM\n *\n */\ninterface ChatReasonFinishedContent {\n    type: 'reasonFinished';\n\n    /**\n     * The id of this reason\n     */\n    id: string; \n\n    /**\n     * The total time the reason took in milliseconds.\n     */\n    totalTimeMs: number;\n}\n\n/**\n * A hook action started to run\n *\n */\ninterface ChatHookActionStartedContent {\n    type: 'hookActionStarted';\n\n    /**\n     * The id of this hook\n     */\n    id: string; \n\n    /**\n     * The name of this hook\n     */\n    name: string;\n\n    /**\n     * The type of this hook action\n     */\n    actionType: 'shell';\n}\n\n/**\n * A hook action finished\n *\n */\ninterface ChatHookActionFinishedContent {\n    type: 'hookActionFinished';\n\n    /**\n     * The id of this hook\n     */\n    id: string; \n\n    /**\n     * The name of this hook\n     */\n    name: string;\n\n    /**\n     * The type of this hook action\n     */\n    actionType: 'shell';\n\n    /**\n     * The status code of this hook\n     */\n    status: number;\n\n    /**\n     * The output of this hook if any\n     */\n    output?: string;\n\n    /**\n     * The error of this hook if any\n     */\n    error?: string;\n}\n\n/**\n * URL content message from the LLM\n */\ninterface ChatURLContent {\n    type: 'url';\n\n    /**\n     * The URL title\n     */\n    title: string;\n\n    /**\n     * The URL link\n     */\n    url: string;\n}\n\n/**\n * Details about the chat's usage, like used tokens and costs.\n */\ninterface ChatUsageContent {\n    type: 'usage';\n\n    /**\n     * The total input + output tokens of the whole chat session so far.\n     */\n    sessionTokens: number;\n\n    /**\n     * The cost of the last sent message summing input + output tokens.\n     */\n    lastMessageCost?: string; \n\n    /**\n     * The cost of the whole chat session so far.\n     */\n    sessionCost?: string;\n\n    /**\n     * Informations about limits.\n     */\n    limit?: {\n        /**\n         * The context limit for this chat.\n         */\n        context: number;\n        /**\n         * The output limit for this chat.\n         */\n        output: number;\n    }\n}\n\n/**\n * Tool call that LLM is preparing to execute.\n * This will be sent multiple times for same tool id for each time LLM outputs \n * a part of the arg, so clients should append the arguments to UI.\n */\ninterface ChatToolCallPrepareContent {\n    type: 'toolCallPrepare';\n\n    origin: ToolCallOrigin;\n\n    /**\n     * id of the tool call\n     */\n    id: string;\n\n    /**\n     * Name of the tool\n     */\n    name: string;\n\n    /**\n     * Server name of this tool\n     */\n    server: string;\n\n    /*\n     * Argument text of this tool call\n     */\n    argumentsText: string; \n\n    /**\n     * Summary text to present about this tool call, \n     * ex: 'Reading file \"foo\"...'.\n     */\n    summary?: string;\n\n    /**\n     * Extra details about this call. \n     * Clients may use this to present different UX for this tool call.\n     */\n    details?: ToolCallDetails;\n}\n\n/**\n * Tool call that LLM will run, sent once per id.\n */\ninterface ChatToolCallRunContent {\n    type: 'toolCallRun';\n\n    origin: ToolCallOrigin;\n\n    /**\n     * id of the tool call\n     */\n    id: string;\n\n    /**\n     * Name of the tool\n     */\n    name: string;\n\n    /**\n     * Server name of this tool\n     */\n    server: string;\n\n    /*\n     * Arguments of this tool call\n     */\n    arguments: {[key: string]: string};\n\n    /**\n     * Whether this call requires manual approval from the user.\n     */\n    manualApproval: boolean;\n\n    /**\n     * Summary text to present about this tool call, \n     * ex: 'Reading file \"foo\"...'.\n     */\n    summary?: string;\n\n    /**\n     * Extra details about this call. \n     * Clients may use this to present different UX for this tool call.\n     */\n    details?: ToolCallDetails;\n}\n\n/**\n * Tool call that server is running to report to LLM later, sent once per id.\n */\ninterface ChatToolCallRunningContent {\n    type: 'toolCallRunning';\n\n    origin: ToolCallOrigin;\n\n    /**\n     * id of the tool call\n     */\n    id: string;\n\n    /**\n     * Name of the tool\n     */\n    name: string;\n\n    /**\n     * Server name of this tool\n     */\n    server: string;\n\n    /*\n     * Arguments of this tool call\n     */\n    arguments: {[key: string]: string};\n\n    /**\n     * Summary text to present about this tool call, \n     * ex: 'Reading file \"foo\"...'.\n     */\n    summary?: string;\n\n    /**\n     * Extra details about this call. \n     * Clients may use this to present different UX for this tool call.\n     */\n    details?: ToolCallDetails;\n}\n\n/**\n * Tool call result that LLM trigerred and was executed already, sent once per id.\n */\ninterface ChatToolCalledContent {\n    type: 'toolCalled';\n\n    origin: ToolCallOrigin;\n\n    /**\n     * id of the tool call\n     */\n    id: string;\n\n    /**\n     * Name of the tool\n     */\n    name: string;\n\n    /**\n     * Server name of this tool\n     */\n    server: string;\n\n    /*\n     * Arguments of this tool call\n     */\n    arguments: string[];\n\n    /**\n     * Whether it was a error\n     */\n    error: boolean;\n\n    /**\n     * the result of the tool call.\n     */\n    outputs: [{\n        /*\n         * The type of this output\n         */\n        type: 'text';\n\n        /**\n         * The content of this output\n         */\n        text: string; \n    }];\n\n    /**\n     * The total time the call took in milliseconds.\n     */\n    totalTimeMs: number;\n\n    /**\n     * Summary text to present about this tool call, \n     * ex: 'Reading file \"foo\"...'.\n     */\n    summary?: string;\n\n    /**\n     * Extra details about this call. \n     * Clients may use this to present different UX for this tool call.\n     */\n    details?: ToolCallDetails;\n}\n\n/**\n * Tool call rejected, sent once per id.\n */\ninterface ChatToolCallRejectedContent {\n    type: 'toolCallRejected';\n\n    origin: ToolCallOrigin;\n\n    /**\n     * id of the tool call\n     */\n    id: string;\n\n    /**\n     * Name of the tool\n     */\n    name: string;\n\n    /**\n     * Server name of this tool\n     */\n    server: string;\n\n    /*\n     * Arguments of this tool call\n     */\n    arguments: {[key: string]: string};\n\n    /**\n     * The reason why this tool call was rejected\n     */\n    reason: 'user-choice' | 'user-config';\n\n    /**\n     * Summary text to present about this tool call, \n     * ex: 'Reading file \"foo\"...'.\n     */\n    summary?: string;\n\n    /**\n     * Extra details about this call. \n     * Clients may use this to present different UX for this tool call.\n     */\n    details?: ToolCallDetails;\n}\n\ntype ToolCallOrigin = 'mcp' | 'native';\n\ntype ToolCallDetails = FileChangeDetails | JsonOutputsDetails;\n\ninterface FileChangeDetails {\n    type: 'fileChange';\n\n     /**\n      * The file path of this file change\n      */\n     path: string;\n\n     /**\n      * The content diff of this file change\n      */\n     diff: string;\n\n     /**\n      * The count of lines added in this change.\n      */\n     linesAdded: number;\n\n     /**\n      * The count of lines removed in this change.\n      */\n     linesRemoved: number;\n}\n\ninterface JsonOutputsDetails {\n    type: 'jsonOutputs';\n\n    /**\n     * The list of json outputs of this tool call properly formatted.\n     */\n    jsons: string[];\n}\n\n/**\n * Extra information about a chat\n */\ninterface ChatMetadataContent {\n    type: 'metadata';\n\n    /**\n     * The chat title.\n     */\n    title: string;\n}\n</code></pre>"},{"location":"protocol/#chat-approve-tool-call","title":"Chat approve tool call (\u27a1\ufe0f)","text":"<p>A client notification for server to approve a waiting tool call. This will execute the tool call and continue the LLM chat loop.</p> <p>Notification:</p> <ul> <li>method: <code>chat/toolCallApprove</code></li> <li>params: <code>ChatToolCallApproveParams</code> defined as follows:</li> </ul> <pre><code>interface ChatToolCallApproveParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId: string;\n\n    /**\n     * The approach to save this tool call.\n     */\n    save?: 'session';\n\n    /**\n     * The tool call identifier to approve.\n     */\n    toolCallId: string; \n}\n</code></pre>"},{"location":"protocol/#chat-reject-tool-call","title":"Chat reject tool call (\u27a1\ufe0f)","text":"<p>A client notification for server to reject a waiting tool call. This will not execute the tool call and return to the LLM chat loop.</p> <p>Notification:</p> <ul> <li>method: <code>chat/toolCallReject</code></li> <li>params: <code>ChatToolCallRejectParams</code> defined as follows:</li> </ul> <pre><code>interface ChatToolCallRejectParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId: string;\n\n    /**\n     * The tool call identifier to reject.\n     */\n    toolCallId: string; \n}\n</code></pre>"},{"location":"protocol/#chat-query-context","title":"Chat Query Context (\u21a9\ufe0f)","text":"<p>A request sent from client to server, querying for all the available contexts for user add to prompt calls.</p> <p>Request: </p> <ul> <li>method: <code>chat/queryContext</code></li> <li>params: <code>ChatQueryContextParams</code> defined as follows:</li> </ul> <pre><code>interface ChatQueryContextParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId?: string;\n\n    /**\n     * The query to filter results, blank string returns all available contexts.\n     */\n    query: string;\n\n    /**\n     * The already considered contexts.\n     */\n    contexts: ChatContext[];\n}\n</code></pre> <p>Response:</p> <pre><code>interface ChatQueryContextResponse {\n    /**\n     * The chat session identifier.\n     */\n    chatId?: string;\n\n    /**\n     * The returned available contexts.\n     */\n    contexts: ChatContext[];\n}\n</code></pre>"},{"location":"protocol/#chat-query-commands","title":"Chat Query Commands (\u21a9\ufe0f)","text":"<p>A request sent from client to server, querying for all the available commands for user to call. Commands are multiple possible actions like MCP prompts, doctor, costs. Usually the  UX follows <code>/&lt;command&gt;</code> to spawn a command.</p> <p>Request: </p> <ul> <li>method: <code>chat/queryCommands</code></li> <li>params: <code>ChatQueryCommandsParams</code> defined as follows:</li> </ul> <pre><code>interface ChatQueryCommandsParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId?: string;\n\n    /**\n     * The query to filter results, blank string returns all available commands.\n     */\n    query: string;\n}\n</code></pre> <p>Response:</p> <pre><code>interface ChatQueryCommandsResponse {\n    /**\n     * The chat session identifier.\n     */\n    chatId?: string;\n\n    /**\n     * The returned available Commands.\n     */\n    commands: ChatCommand[];\n}\n\ninterface ChatCommand {\n    /**\n     * The name of the command.\n     */\n    name: string;\n\n    /**\n     * The description of the command.\n     */\n    description: string;\n\n    /**\n     * The type of this command\n     */\n    type: 'mcp-prompt' | 'native';\n\n    /**\n     * The arguments of the command.\n     */\n    arguments: [{\n       name: string;\n       description?: string;\n       required: boolean; \n    }];\n}\n</code></pre>"},{"location":"protocol/#chat-stop-prompt","title":"Chat stop prompt (\u27a1\ufe0f)","text":"<p>A client notification for server to stop the current chat prompt with LLM if running. This will stop LLM loops or ignore subsequent LLM responses so other prompts can be trigerred.</p> <p>Notification:</p> <ul> <li>method: <code>chat/promptStop</code></li> <li>params: <code>ChatPromptStopParams</code> defined as follows:</li> </ul> <pre><code>interface ChatPromptStopParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId: string;\n}\n</code></pre>"},{"location":"protocol/#chat-rollback","title":"Chat rollback (\u21a9\ufe0f)","text":"<p>A client request to rollback chat messages to before a specific user sent message using <code>contentId</code>. Clients should show an option close to user sent messages in chat to rollback, calling this method. Server will then remove the messages from its memory after that contentId and produce <code>chat/cleared</code> followed  with <code>chat/contentReceived</code> with the kept messages.</p> <p>Request: </p> <ul> <li>method: <code>chat/rollback</code></li> <li>params: <code>ChatRollbackParams</code> defined as follows:</li> </ul> <pre><code>interface ChatRollbackParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId: string;\n\n    /**\n     * The message content id.\n     */\n    contentId: string;\n\n    /**\n     * The types of rollbacks to include, allowing to rollback one or more types.\n     */\n    include: ChatRollbackInclude[];\n}\n\ntype ChatRollbackInclude = 'messages' | 'tools';\n</code></pre> <p>Response:</p> <pre><code>interface ChatRollbackResponse {}\n</code></pre>"},{"location":"protocol/#chat-cleared","title":"Chat cleared (\u2b05\ufe0f)","text":"<p>A server notification to clear a chat UI, currently supporting removing only messages of the chat.</p> <p>Request: </p> <ul> <li>method: <code>chat/cleared</code></li> <li>params: <code>ChatClearedParams</code> defined as follows:</li> </ul> <pre><code>interface ChatClearedParams {\n\n    /**\n     * The chat session identifier.\n     */\n    chatId: string;\n\n    /**\n     * Whether to clear the messages of a chat.\n     */\n    messages: boolean;\n}\n</code></pre>"},{"location":"protocol/#chat-delete","title":"Chat delete (\u21a9\ufe0f)","text":"<p>A client request to delete a existing chat, removing all previous messages and used tokens/costs from memory, good for reduce context or start a new clean chat. After response, clients should reset chat UI to a clean state.</p> <p>Request: </p> <ul> <li>method: <code>chat/delete</code></li> <li>params: <code>ChatDeleteParams</code> defined as follows:</li> </ul> <pre><code>interface ChatDeleteParams {\n    /**\n     * The chat session identifier.\n     */\n    chatId?: string;\n}\n</code></pre> <p>Response:</p> <pre><code>interface ChatDeleteResponse {}\n</code></pre>"},{"location":"protocol/#chat-selected-behavior-changed","title":"Chat selected behavior changed (\u27a1\ufe0f)","text":"<p>A client notification for server telling the user selected a different behavior in chat.</p> <p>Notification:</p> <ul> <li>method: <code>chat/selectedBehaviorChanged</code></li> <li>params: <code>ChatSelectedBehaviorChanged</code> defined as follows:</li> </ul> <pre><code>interface ChatSelectedBehaviorChanged {\n    /**\n     * The selected behavior.\n     */\n    behavior: ChatBehavior;\n}\n</code></pre>"},{"location":"protocol/#editor-diagnostics","title":"Editor diagnostics (\u21aa\ufe0f)","text":"<p>A server request to retrieve LSP or any other kind of diagnostics if available from current workspaces. Useful for server to provide to LLM information about errors/warnings about current code.</p> <p>Request: </p> <ul> <li>method: <code>editor/getDiagnostics</code></li> <li>params: <code>EditorGetDiagnosticsParams</code> defined as follows:</li> </ul> <pre><code>interface EditorGetDiagnosticsParams {\n    /**\n     * Optional uri to get diagnostics, if nil return whole workspaces diagnostics.\n     */\n    uri?: string;\n}\n</code></pre> <p>Response:</p> <pre><code>interface EditorGetDiagnosticsResponse {\n    /**\n     * The list of diagnostics.\n     */\n    diagnostics: EditorDiagnostic[];\n}\n\ninterface EditorDiagnostic {\n    /**\n     * The diagnostic file uri.\n     */\n    uri: string;\n\n    /**\n     * The diagnostic severity.\n     */\n    severity: 'error' | 'warning' | 'info' | 'hint';\n\n    /**\n     * The diagnostic source. Ex: 'clojure-lsp'\n     */\n    source: string;\n\n    /**\n     * The diagnostic range (1-based).\n     */\n    range: Range;\n\n    /**\n     * The diagnostic code. Ex: 'wrong-args'\n     */\n    code?: string;\n\n    /**\n     * The diagnostic message. Ex: 'Wrong number of args for function X'\n     */\n    message: string; \n}\n</code></pre>"},{"location":"protocol/#completion","title":"Completion (\u21a9\ufe0f)","text":"<p>A request sent from client to server, asking for a text to be presented to user as a inline completion for the current text code.</p> <p>Request: </p> <ul> <li>method: <code>completion/inline</code></li> <li>params: <code>CompletionInlineParams</code> defined as follows:</li> </ul> <pre><code>interface CompletionInlineParams {\n    /**\n     * The current document text.\n     */\n    docText: string;\n\n    /**\n     * The document version.\n     * Clients should increment this on their side each time document is changed.\n     * Server will return this on its completion so clients can \n     * discard if document was changed/version was increased.\n     */\n    docVersion: number;\n\n    /**\n     * The cursor position.\n     */\n    position: {\n       line: number;\n       character: number;\n    };\n}\n</code></pre> <p>Response: <code>CompletionInlineResponse | Error</code></p> <pre><code>interface CompletionInlineResponse {\n    /**\n     * The items available as completion.\n     */\n    items: CompletionInlineItem[];\n}\n\ninterface CompletionInlineItem {\n    /**\n     * The item text\n     */\n    text: string;\n\n    /**\n     * The item doc-version\n     */\n    docVersion: number;\n\n    /**\n     * The range of this new text in the document\n     */\n    range: Range;\n}\n</code></pre>"},{"location":"protocol/#rewrite","title":"Rewrite (\u21a9\ufe0f)","text":"<p>A request sent from client to server, asking to rewrite a piece of code in editor with what LLM respond. The response is streamed via <code>rewrite/contentReceived</code> server notifications.</p> <p>Request: </p> <ul> <li>method: <code>rewrite/prompt</code></li> <li>params: <code>RewritePromptParams</code> defined as follows:</li> </ul> <pre><code>interface RewritePromptParams {\n\n    /**\n     * The rewrite ID to be used in response and later notifications.\n     */\n    id: string;\n\n    /**\n     * The text to be rewritten.\n     * This should be the content selected by user in their editor.\n     */\n    text: string;\n\n    /**\n     * The user prompt to LLM change the text.\n     */\n    prompt: string;\n\n    /**\n     * Optional path of the file.\n     * Useful for give context to LLM about the file path.\n     */\n    path?: string;\n\n    /**\n     * The range of the selected text.\n     */\n    range: Range;\n}\n</code></pre> <p>Response: <code>RewritePromptResponse | Error</code></p> <pre><code>interface RewritePromptResponse {\n\n    /**\n     * The status of this rewrite.\n     */\n    status: 'prompting';\n\n    /**\n     * The model used by this rewrite request.\n     */\n    model: Model;\n}\n</code></pre>"},{"location":"protocol/#rewrite-content-received","title":"Rewrite Content Received (\u2b05\ufe0f)","text":"<p>A server notification with a new content from the rewrite LLM request.</p> <p>Notification: </p> <ul> <li>method: <code>rewrite/contentReceived</code></li> <li>params: <code>RewriteContentReceivedParams</code> defined as follows:</li> </ul> <pre><code>interface RewriteContentReceivedParams {\n    /**\n     * The rewrite identifier this content belongs to\n     */\n    rewriteId: string;\n\n    /**\n     * The content received\n     */\n    content: RewriteContent;\n}\n\ninterface RewriteStartedContent {\n    type: 'started';\n}\n\ninterface RewriteReasoningContent {\n    type: 'reasoning';\n}\n\ninterface RewriteTextContent {\n    type: 'text';\n\n    text: string;\n}\n\ninterface RewriteErrorContent {\n    type: 'error';\n\n    message: string;\n}\n\ninterface RewriteFinishedContent {\n    type: 'finished';\n}\n\ntype RewriteContent = \n    RewriteStartedContent\n    | RewriteReasoningContent\n    | RewriteTextContent\n    | RewriteErrorContent\n    | RewriteFinishedContent;\n</code></pre>"},{"location":"protocol/#configuration","title":"Configuration","text":""},{"location":"protocol/#config-updated","title":"Config updated (\u2b05\ufe0f)","text":"<p>A server notification with the new config server is considering (models, behaviors etc), usually related to config or auth changes. Clients should update UI accordingly, if a field is missing/null, means it had no change since last config updated, so clients should ignore.</p> <p>Notification: </p> <ul> <li>method: <code>config/updated</code></li> <li>params: <code>configUpdatedParams</code> defined as follows:</li> </ul> <pre><code>interface ConfigUpdatedParams {\n    /**\n     * Configs related to chat.\n     */\n    chat?: {\n\n       /**\n        * The models the user can use in chat.\n        */\n        models?: Model[];\n\n        /**\n        * The chat behaviors the user can select.\n        */\n        behaviors?: ChatBehavior[];\n\n        /**\n         * The model for client select in chat, if that is present\n         * clients should forcefully update chat selected model.\n         * \n         * Server returns this when starting and only when makes sense to \n         * force update a model, like a config change.\n         */\n        selectModel?: Model;\n\n        /**\n         * The behavior for client select in chat, if that is present\n         * clients should forcefully update chat selected behavior.\n         * \n         * Server returns this when starting and only when makes sense to \n         * force update a behavior, like a config change.\n         */\n        selectBehavior?: ChatBehavior;\n\n        /**\n        * Message to show when starting a new chat.\n        */\n        welcomeMessage?: string;\n    }\n}\n</code></pre>"},{"location":"protocol/#tool-updated","title":"Tool updated (\u2b05\ufe0f)","text":"<p>A server notification about a tool status update like a MCP or native tool. This is useful for clients present to user the list of configured tools/MCPs, their status and available tools and actions.</p> <p>Notification: </p> <ul> <li>method: <code>tool/serverUpdated</code></li> <li>params: <code>ToolServerUpdatedParams</code> defined as follows:</li> </ul> <pre><code>type ToolServerUpdatedParams = EcaServerUpdatedParams | MCPServerUpdatedParams;\n\ninterface EcaServerUpdatedParams {\n    type: 'native';\n\n    name: 'ECA';\n\n    status: 'running';\n\n    /**\n     * The built-in tools supported by eca.\n     */\n    tools: ServerTool[];\n}\n\ninterface MCPServerUpdatedParams {\n    type: 'mcp';\n\n    /**\n     * The server name.\n     */\n    name: string;\n\n    /**\n     * The command to start this server.\n     */\n    command: string;\n\n    /**\n     * The arguments to start this server.\n     */\n    args: string[];\n\n    /**\n     * The status of the server.\n     */\n    status: 'running' | 'starting' | 'stopped' | 'failed' | 'disabled';\n\n    /**\n     * The tools supported by this mcp server if not disabled.\n     */\n    tools?: ServerTool[];\n}\n\ninterface ServerTool {\n    /**\n     * The server tool name.\n     */\n    name: string;\n\n    /**\n     * The server tool description.\n     */\n    description: string;\n\n    /**\n     * The server tool parameters.\n     */\n    parameters: any; \n\n    /**\n     * Whther this tool is disabled.\n     */\n    disabled?: boolean;\n}\n</code></pre>"},{"location":"protocol/#stop-mcp-server","title":"Stop MCP server (\u27a1\ufe0f)","text":"<p>A client notification for server to stop a MCP server, stopping the process. Updates its status via <code>tool/serverUpdated</code> notification.</p> <p>Notification:</p> <ul> <li>method: <code>mcp/stopServer</code></li> <li>params: <code>MCPStopServerParams</code> defined as follows:</li> </ul> <pre><code>interface MCPStopServerParams {\n    /**\n     * The MCP server name.\n     */\n    name: string;\n}\n</code></pre>"},{"location":"protocol/#start-mcp-server","title":"Start MCP server (\u27a1\ufe0f)","text":"<p>A client notification for server to start a stopped MCP server, starting the process again. Updates its status via <code>tool/serverUpdated</code> notification.</p> <p>Notification:</p> <ul> <li>method: <code>mcp/startServer</code></li> <li>params: <code>MCPStartServerParams</code> defined as follows:</li> </ul> <pre><code>interface MCPStartServerParams {\n    /**\n     * The server name.\n     */\n    name: string;\n}\n</code></pre>"},{"location":"protocol/#add-mcp","title":"Add MCP (\u21a9\ufe0f)","text":"<p>Soon</p>"},{"location":"protocol/#general-features","title":"General features","text":""},{"location":"protocol/#showmessage","title":"showMessage (\u2b05\ufe0f)","text":"<p>A notification from server telling client to present a message to user.</p> <p>Request: </p> <ul> <li>method: <code>$/showMessage</code></li> <li>params: <code>ShowMessageParams</code> defined as follows:</li> </ul> <pre><code>type ShowMessageParams = Error;\n</code></pre>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#logs-stderr","title":"Logs (stderr)","text":"<p>All supported editors have options to set the server args to help with that and options to check the server logs.</p> <p>To access the server logs:</p> EmacsVsCodeIntelliJNvim <p><code>M-x</code> <code>eca-show-stderr</code></p> <p>Check the output channel <code>ECA stderr</code>.</p> <p>Via action 'ECA: Show server logs'.</p> <p><code>EcaShowLogs</code> </p>"},{"location":"troubleshooting/#server-logs","title":"Server logs","text":"<p>This controls what's logged by server on its actions, you can control to log more things via <code>--log-level debug</code> server arg. This should help log LLM outputs, and other useful stuff.</p>"},{"location":"troubleshooting/#client-server-logs","title":"Client&lt;-&gt;Server logs","text":"<p>ECA works with clients (editors) sending and receiving messages to server, a process, you can start server <code>--verbose</code> which should log all jsonrpc communication between client and server to <code>stderr</code> buffer like what is being sent to LLMs or what ECA is responding to editors. </p>"},{"location":"troubleshooting/#doctor-command","title":"Doctor command","text":"<p><code>/doctor</code> command should log useful information to debug model used, server version, env vars and more.</p>"},{"location":"troubleshooting/#missing-env-vars","title":"Missing env vars","text":"<p>When launching editors from a GUI application (Dock, Applications folder, or desktop environment), high chance that it won't inherit environment variables from your shell configuration files (<code>.zshrc</code>, <code>.bashrc</code>, etc.). Since the ECA server is started as a subprocess from editor, it inherits the editor environment, which may be missing your API keys and other configuration.</p> <p>You can check if the env vars are available via <code>/doctor</code>.</p> <p>One way to workaround that is to start the editor from your terminal.</p>"},{"location":"troubleshooting/#alternatives","title":"Alternatives","text":"<ul> <li> <p>Start the editor from your terminal.</p> </li> <li> <p>Set variables in your editor if supported, example in Emacs: <code>(setenv \"MY_ENV\" \"my-value\")</code></p> </li> <li> <p>On macOS, you can set environment variables system-wide using <code>launchctl</code>:</p> </li> </ul> <pre><code>launchctl setenv ANTHROPIC_API_KEY \"your-key-here\"\n</code></pre>"},{"location":"troubleshooting/#ask-for-help","title":"Ask for help","text":"<p>You can ask for help via chat here</p>"}]}